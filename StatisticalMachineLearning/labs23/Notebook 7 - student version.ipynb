{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SisIDNTm3KNb"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\">\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'>\n",
    "\n",
    "\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\".   \n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical machine learning - Notebook 7, version for students\n",
    "**Author: Michał Ciach**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAMEuAbbH-Or"
   },
   "source": [
    "## Description\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXweAU-UR7F7"
   },
   "source": [
    "In today's class, we will learn the basics of parameter estimation in the Bayesian framework. In contrast to the frequentist approach that we've seen so far, the Bayesian one assumes that the unknown parameters (such as the mean of the data) are random variables. This is a useful approach to express the uncertainty of estimation.  \n",
    "\n",
    "Formally, we assume that our data is a sample *conditional* on a parameter value, $X_1, \\dots X_n | \\theta$. If we know the value of $\\theta$, we can write the probability density function of the data. In the context of Bayesian statistics, we refer to this function as the *likelihood*. For example, for a random sample from a normal distribution with an unknown mean $\\mu$ and a known variance $\\sigma^2$ we would write $X_1, \\dots, X_n | \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The parameters $\\theta$ have their own *prior* distribution that's independent on the data. For example, we may assume that $\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. The distribution of the parameters conditional on the data is called the *posterior distribution* and is the result of estimation in the Bayesian framework. In our example, it turns out that\n",
    "\n",
    "$$ \\mu | X_1, \\dots, X_n \\sim \\mathcal{N}\\left(\\frac{\\sigma^{-2}_0\\mu_0 + \\sigma^{-2}\\sum X_i }{\\sigma_0^{-2} + n\\sigma^{-2}}, \\frac{1}{\\sigma_0^{-2} + n\\sigma^{-2}}\\right).$$ \n",
    "\n",
    "The expected value of the parameter under it's posterior distribution is the *Bayesian estimator* of this parameter. In our case, the Bayesian estimator of the mean is equal to $\\frac{\\sigma^{-2}_0\\mu_0 + \\sigma^{-2}\\sum X_i }{\\sigma_0^{-2} + n\\sigma^{-2}}$. \n",
    "\n",
    "The probability distribution of a new data point conditioned on the observed data is called the *posterior predictive*. It's used to predict the values of new observations based on the data. Formally, it's equal to\n",
    "$$p(\\tilde{x} | x_1, \\dots, x_n) = \\int L(\\tilde{x} | \\theta) \\pi (\\theta | x_1, \\dots, x_n) d\\theta.$$\n",
    "Basically, this is the likelihood function combined with the posterior distribution of $\\theta$. It can be interpreted as a *fitted bayesian model* and used to roughly check whether we have specified our model correctly. However, it's usually broader than the observed distribution of the data, which reflects the uncertainty of prediction. In our case, the posterior predictive is \n",
    "\n",
    "$$\\tilde{X} \\sim \\mathcal{N}(\\mu_0', \\sigma_0'^{2} + \\sigma^2),$$  \n",
    "\n",
    "where $\\mu_0'$ and $\\sigma_0'^2$ are the *posterior* mean and variance of $\\mu$ (i.e. $\\mu_0'$ is the Bayesian estimator of the mean).  \n",
    "\n",
    "In general, we can set any distribution as the prior. However, there is a particularly useful type called a *conjugate prior*. Rougly speaking, this is the type of prior that we get if we treat the likelihood as a probability density function for the parameter. \n",
    "\n",
    "Here is an example of constructing a conjugate prior. If we have a simple observation $X_1 | \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the likelihood is equal to\n",
    "\n",
    "$$L(x_1 | \\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(x_1 - \\mu)^2}{2\\sigma^2}\\right).$$\n",
    "Now, we treat it as a function of $\\mu$ and replace all other parameters with new ones. Let's replace $x_1$ with $\\mu_0$ and $\\sigma$ with $\\sigma_0$. This gives us\n",
    "\n",
    "$$\\tilde{\\pi}(\\mu | \\mu_0, \\sigma_0) = (2\\pi\\sigma_0^2)^{-\\frac{1}{2}}\\exp\\left(-\\frac{(\\mu_0 - \\mu)^2}{2\\sigma_0^2}\\right).$$\n",
    "\n",
    "This means that the conjugate prior for $X_1 | \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2)$, when $\\sigma^2$ is known, is also a normal distribution $\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. If we have more data points, we do a similar procedure, but we need to remember to start from the *joint likelihood of the whole sample.*\n",
    "\n",
    "Often, simply replacing parameters will give us a function that does not integrate to 1, i.e. is not a probability density. To get a conjugate prior in this case, we strip all factors that do not depend on $\\mu$ from the likelihood, replace all the remaining parameters (including the data) with new ones, and read out what kind of distribution we get based on the parts that depend on $\\mu$. In our example, the only part that depends on $\\mu$ is $\\exp(-(\\mu-\\mu_0)^2/(2\\sigma_0^2))$, which means that this distribution is gaussian. \n",
    "\n",
    "The parameters of the prior distribution, in our case $\\mu_0$ and $\\sigma_0$, are assumed to be non-random. They're called the *hyperparameters* to distinguished them from the random ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5786,
     "status": "ok",
     "timestamp": 1638115726509,
     "user": {
      "displayName": "Michał Ciach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpPBEe8EA8vnqB1IFPaB2zFKwgD2JhgBbLleka=s64",
      "userId": "02255227725825219408"
     },
     "user_tz": -60
    },
    "id": "jkB6U888K8KX",
    "outputId": "c962d6b4-1ac7-4b43-a62a-2fd47d8bd019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GW1pjKOCoKOlC4Jqbqql_ghYD_n0iC6O\n",
      "To: /content/BDL municipality incomes 2015-2020.csv\n",
      "100% 228k/228k [00:00<00:00, 82.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FInZ2jrlZGNColU4sHF9JKGHP39fTVut\n",
      "To: /content/BDL municipality area km2 2015-2020.csv\n",
      "100% 180k/180k [00:00<00:00, 65.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1n1qS6dcVVKcVJOuUIIm0VTz6cSyrtzDH\n",
      "To: /content/BDL municipality population 2015-2020.csv\n",
      "100% 222k/222k [00:00<00:00, 32.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1GW1pjKOCoKOlC4Jqbqql_ghYD_n0iC6O\n",
    "!gdown https://drive.google.com/uc?id=1FInZ2jrlZGNColU4sHF9JKGHP39fTVut\n",
    "!gdown https://drive.google.com/uc?id=1n1qS6dcVVKcVJOuUIIm0VTz6cSyrtzDH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbgBtcAsK6T2"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8H2cI48aR97y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from scipy.stats import norm, lognorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dofE7aHSGpk"
   },
   "outputs": [],
   "source": [
    "income = pd.read_csv('BDL municipality incomes 2015-2020.csv', sep=';', dtype={'Code': 'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4PYWS9CRvKI"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2GdZaHuSNGs"
   },
   "source": [
    "In Notebook 2, we have encountered the *variance-bias tradeoff*, where a biased estimator (a geometric mean) gave better results compared to an unbiased one (an arithmentic mean), because it reduced the variance of the estimation. \n",
    "\n",
    "Bayesian statistics is another approach to deal with the variance-bias tradeoff. It allows us to limit both the bias and the variance of the estimator by setting an appropriate prior distribution.  \n",
    "\n",
    "How do we set an appropriate prior and its hyperparameters? We guess. \n",
    "Usually we have some prior knowledge that we can use---for example, we know that the population of a municipality is probably somewhere between 10 people and 10 million people.  \n",
    "The better our guess, the better the results. As long as we don't make a terrible mistake, we'll be fine.  \n",
    "\n",
    "Using the random sample to estimate the prior parameters is not a good practice, because it underestimates the uncertainty of estimation. It's better to set very general priors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkkta6d4Sp4k"
   },
   "source": [
    "**Exercise 1.** In the first exercise, we'll perform a bayesian estimation of the mean income of municipalities based on a random sample. The sample is already selected in the cell below. For simplicity, we'll assume that the income and its mean are normally distributed, and that the frequentist estimator of the standard deviation always gives us the correct answer (so that we can use a model with a known variance).  \n",
    "\n",
    "First, using the $3\\sigma$ rule, calculate the hyperparameters for priors that assume:\n",
    "1. 99% probability that the mean income is between $10^4$ and $10^{12}$ PLN (a *weakly informative* prior),\n",
    "2. 99% probability that the mean income is between $10^6$ and $2\\cdot 10^8$ PLN (a *moderately informative* prior),\n",
    "3. 99% probability that the mean income is between $4\\cdot 10^7$ and $6 \\cdot 10^7$ PLN (a *strongly informative* prior),\n",
    "4. 99% probability that the mean income is between $8 \\cdot 10^7$ and $10^8$ PLN (a strongly informative, but *incorrect* prior).\n",
    "\n",
    "Write a function that takes the prior parameters, the mean and standard deviation estimated from the random sample, and the size of the sample, and returns the hyperparameters of the posterior distribution (the posterior mean and standard deviation). You can use the formulas from the description of this notebook or look them up at the [Wikipedia article](https://en.wikipedia.org/wiki/Conjugate_prior).\n",
    "\n",
    "Using the `norm.pdf` function, compute the posterior probability densities in points given by `x = np.linspace(1e06, 2e08, 501)` for all four priors. Visualize the densities on a plot. Annotate the plot with the true mean income and the value of the frequentist estimator (i.e. the arithmetic mean of the sample). Hint: create a data frame `posterior_pdf = pd.DataFrame({'x': x})` and add columns with the computed density values. Next, use `posterior_pdf = posterior_pdf.melt(id_vars='x', var_name=\"Type of prior\")` to get the data in a format suitable for plotting with `plotly.express`. Use the `fig = px.line()` function for plotting and `fig.add_vline()` to annotate the plots.  \n",
    "\n",
    "Create a plot showing the probability density function of the moderately informative prior and the corresponding posterior. Answer the following questions: How did the sample influence the prior distributions? Is there a large difference between the posteriors for the weakly and the moderately informative priors? What is the effect of incorrectly specifying the prior compared to specifying a prior with a large variance?\n",
    "\n",
    "What happens if you increase the size of the sample?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1638115727293,
     "user": {
      "displayName": "Michał Ciach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpPBEe8EA8vnqB1IFPaB2zFKwgD2JhgBbLleka=s64",
      "userId": "02255227725825219408"
     },
     "user_tz": -60
    },
    "id": "9Swg2rl0SNXO",
    "outputId": "e0820245-05d1-4aa3-debd-0094569972cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: 49129101 and standard deviation: 300858590\n",
      "Estimated mean: 51173876 and standard deviation: 111900180\n"
     ]
    }
   ],
   "source": [
    "## Get the data:\n",
    "income2020 = income['2020'].dropna()\n",
    "true_mean, true_sd = income2020.mean(), income2020.std()\n",
    "print('True mean:', round(true_mean), 'and standard deviation:', round(true_sd))\n",
    "## Get the sample:\n",
    "N = 36\n",
    "#income_sample = income2020.sample(N)\n",
    "income_sample = income2020[[2241, 1980, 2436,  979, 1064, 2146, 1983,  464, 1262,  318, 2429,\n",
    "                            1609, 2320, 1383,  813, 1948, 2392, 1930, 1751, 1330, 1586,  856,\n",
    "                            1149, 2369, 2189, 1993, 1911,  225,  546,  843, 1389,  821,  338,\n",
    "                            1986, 1132, 1077]]\n",
    "## Frequentist estimate:\n",
    "mu_estim = income_sample.mean()\n",
    "sd_estim = income_sample.std()\n",
    "print('Estimated mean:', round(mu_estim), 'and standard deviation:', round(sd_estim))\n",
    "## Write the rest of your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDanBr5nzO-y"
   },
   "source": [
    "**Exercise 2.** In the last exercise, we've built a Bayesian model in which we've assumed that the data is normally distributed. However, we've already seen in the previous classes that this is not the case. In this exercise, we'll investigate the effect of assuming a wrong distribution on the estimation.  \n",
    "\n",
    "Calculate the values of the probability density of the posterior predictive distribution (remember: the *posterior distribution* is about the parameters, the *posterior predictive distribution* is about the data!). Using `fig4.add_scatter()`, draw the distributions over the histogram generated in the cell below. Inspect the quality of fit (use the zoom functionality of Plotly to zoom in the low-income part).  \n",
    "\n",
    "We will check how this assumption influences the root mean squared error of the estimators. The RMSE for a sample of estimator values $\\hat{\\mu}_1, \\dots, \\hat{\\mu_N}$ is given by the formula \n",
    "\n",
    "$$RMSE = \\sqrt{\\sum_{i=1}^N (\\hat{\\mu}_i - \\mu)^2},$$\n",
    "and is an approximation of the average error that we make on a single estimation.  \n",
    "\n",
    "Estimate the root mean squared error (RMSE) of the frequentist estimator and bayesian estimators for two priors of your choice.\n",
    "Compute arithmetic means and the means of the posterior distributions for 1000 independent samples of municipality incomes. Visualize them on histograms annotated with the true mean incomes. Compute the RMSE. Did the frequentist estimator outperform the Bayesian ones because of the improper assumption about the distribution of the data? Can you see the influence of the prior distribution on the distribution of the Bayesian estimator?\n",
    "\n",
    "What is the posterior distribution of the mean and the Bayesian estimator in the case of a non-informative prior ($\\sigma_0^{-2} = 0$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvzZJJ1o1oE1"
   },
   "outputs": [],
   "source": [
    "fig4 = px.histogram(x=income2020, histnorm='probability density', nbins=601)\n",
    "x = np.linspace(0, max(income2020), 1001)\n",
    "\n",
    "## Write the rest of your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpGxJzwyWyIo"
   },
   "source": [
    "**Exercise 3.** In the previous classes, we saw that the distribution of the log-income resembles the normal distribution. This means that for the raw income, the [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution) distribution is probably a better fit than the normal one. The log-normal distribution $Y \\sim log\\mathcal{N}(\\mu, \\sigma^2)$ is defined as the natural exponent of a normally distributed variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, i.e. $Y = e^X$. It's probability density function is given by\n",
    "\n",
    "$$f(y) = \\frac{1}{y\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(\\ln(y) - \\mu)^2}{2\\sigma^2} \\right).$$\n",
    "\n",
    "In this exercise, we'll check how well the log-normal distribution fits the income data and whether a better-fitting distribution allows for a better estimation of the mean.  \n",
    "\n",
    "First, check what is the distribution of the conjugate prior for the log-mean parameter $\\mu$. Next, derive its posterior distribution. Remember to use the likelihood for the whole sample, not just a single data point.  \n",
    "\n",
    "Now, we need to figure out how to estimate the mean income in this model. Note that the $\\mu$ parameter in this case is the mean log-income (the mean value of $\\ln (Y)$), which, as we've already seen in Notebook 4, is much different than the logarithm of the mean income: $e^\\mu \\neq \\bar{Y}$. In order to estimate the mean value of the raw income, $\\bar{Y}$, we can use the posterior predictive distribution. \n",
    "\n",
    "The posterior predictive distribution of $Y$ in our case (assuming you've found the correct conjugate prior) is equal to\n",
    "\n",
    "$$Y^* \\sim log\\mathcal{N}(\\mu_0', \\sigma_0'^2 + \\sigma^2),$$\n",
    "\n",
    "where $\\sigma^2$ is the true variance of the log-income (for simplicity, as in the previous exercises, we assume that it's equal to the variance estimated from the sample). Find the expected value of $Y^*$ (look for appropriate formulas for the log-normal distribution on Wikipedia) and use it to estimate the mean income.  \n",
    "\n",
    "Compute the parameters for the following priors for the mean of the log-income (note that we're using *natural* logarithms in this exercise):   \n",
    "1. 99% probability that the mean log-income is between 10 and 25\n",
    "2. 99% probability that the mean log-income is between 12 and 20\n",
    "3. 99% probability that the mean log-income is between 16 and 17\n",
    "4. 99% probability that the mean log-income is between 18 and 19\n",
    "\n",
    "Compute the posterior parameters for all four priors based on the random sample from Exercise 1. Visualize the densities of the posterior distributions on a plot and annotate it with the true value of the mean log-income and the frequentist estimate. Is there any difference compared to the posteriors from Exercise 1? Create a plot showing a selected prior and the corresponding posterior and interpret it in terms of how the data influenced the distribution of the parameter.   \n",
    "\n",
    "Visualize the posterior predictive distributions for the four priors and compare them to the histogram of the data. Does this model fit better?\n",
    "\n",
    "Estimate the RMSE of the estimators of the mean income $\\bar{Y}$ for the weakly informative prior and the improper prior. Compare it to the RMSE values from the previous exercise. Which estimator works best? What is the effect of an improper prior when the probability distributions of the model are properly defined? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1638128715463,
     "user": {
      "displayName": "Michał Ciach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhpPBEe8EA8vnqB1IFPaB2zFKwgD2JhgBbLleka=s64",
      "userId": "02255227725825219408"
     },
     "user_tz": -60
    },
    "id": "CR2m3T1LF9PG",
    "outputId": "d0882237-2f86-462d-895c-ef295f796a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True log-income mean: 16.788 with sd 1.0086\n",
      "Estimated log-income mean: 16.9623 with sd 1.0638\n"
     ]
    }
   ],
   "source": [
    "## Get the true mean and sd of log-incomes:\n",
    "true_log_mu = np.log(income2020).mean()\n",
    "true_log_sd = np.log(income2020).std()\n",
    "## Get the estimates from a sample:\n",
    "mu_log_estim = np.log(income_sample).mean()\n",
    "sd_log_estim = np.log(income_sample).std()\n",
    "print('True log-income mean:', round(true_log_mu, 4), 'with sd', round(true_log_sd, 4))\n",
    "print('Estimated log-income mean:', round(mu_log_estim, 4), 'with sd', round(sd_log_estim, 4))\n",
    "\n",
    "## Write the rest of your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RPazcz6RiIi"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\">\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'>\n",
    "\n",
    "\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\".   \n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKXbmsYyyA8++usyFMcTkF",
   "collapsed_sections": [],
   "name": "Notebook 7 - student version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
