{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SisIDNTm3KNb"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?export=view&id=12CrUdXDAiltLBT26sG7HZ_HciIhvGyT8'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical machine learning - Notebook 9, version for students\n",
    "**Author: Michał Ciach**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAMEuAbbH-Or"
   },
   "source": [
    "## Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSgU_ssarko8"
   },
   "source": [
    "In today's class, we will analyze the properties of $k$-fold cross validation, the Bootstrap, and feature selection techniques.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkB6U888K8KX",
    "outputId": "735f5503-6ca5-4bae-fab7-221471251e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.11/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "/bin/bash: linia 1: gdown: nie znaleziono polecenia\n",
      "/bin/bash: linia 1: gdown: nie znaleziono polecenia\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1GW1pjKOCoKOlC4Jqbqql_ghYD_n0iC6O\n",
    "!gdown https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbgBtcAsK6T2"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8H2cI48aR97y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import ttest_ind, pearsonr, norm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "CKTr3Kkfb2-f",
    "outputId": "ebc949e0-e5cb-4a6f-806f-3928945e5f1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scientific name</th>\n",
       "      <th>Common name</th>\n",
       "      <th>Protein ID</th>\n",
       "      <th>Protein length</th>\n",
       "      <th>LogLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>1474</td>\n",
       "      <td>3.168497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>290</td>\n",
       "      <td>2.462398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>421</td>\n",
       "      <td>2.624282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000008.1</td>\n",
       "      <td>412</td>\n",
       "      <td>2.614897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000009.1</td>\n",
       "      <td>655</td>\n",
       "      <td>2.816241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648731</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560453.1</td>\n",
       "      <td>494</td>\n",
       "      <td>2.693727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648732</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560454.1</td>\n",
       "      <td>737</td>\n",
       "      <td>2.867467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648733</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560455.1</td>\n",
       "      <td>554</td>\n",
       "      <td>2.743510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648734</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560456.1</td>\n",
       "      <td>813</td>\n",
       "      <td>2.910091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648735</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560457.1</td>\n",
       "      <td>102</td>\n",
       "      <td>2.008600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648736 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Scientific name            Common name    Protein ID  Protein length  \\\n",
       "0         Homo sapiens                  Human   NP_000005.3            1474   \n",
       "1         Homo sapiens                  Human   NP_000006.2             290   \n",
       "2         Homo sapiens                  Human   NP_000007.1             421   \n",
       "3         Homo sapiens                  Human   NP_000008.1             412   \n",
       "4         Homo sapiens                  Human   NP_000009.1             655   \n",
       "...                ...                    ...           ...             ...   \n",
       "648731   Imleria badia  Bay bolete (mushroom)  KAF8560453.1             494   \n",
       "648732   Imleria badia  Bay bolete (mushroom)  KAF8560454.1             737   \n",
       "648733   Imleria badia  Bay bolete (mushroom)  KAF8560455.1             554   \n",
       "648734   Imleria badia  Bay bolete (mushroom)  KAF8560456.1             813   \n",
       "648735   Imleria badia  Bay bolete (mushroom)  KAF8560457.1             102   \n",
       "\n",
       "        LogLength  \n",
       "0        3.168497  \n",
       "1        2.462398  \n",
       "2        2.624282  \n",
       "3        2.614897  \n",
       "4        2.816241  \n",
       "...           ...  \n",
       "648731   2.693727  \n",
       "648732   2.867467  \n",
       "648733   2.743510  \n",
       "648734   2.910091  \n",
       "648735   2.008600  \n",
       "\n",
       "[648736 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_lengths = pd.read_csv('protein_lengths.tsv', sep='\\t')\n",
    "protein_lengths['LogLength'] = np.log10(protein_lengths['Protein length'])\n",
    "protein_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "UlgsjbElb5t2",
    "outputId": "635da691-6db4-4954-df20-91f4bd431a93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein length</th>\n",
       "      <th>LogLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>136193.000000</td>\n",
       "      <td>136193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>692.655775</td>\n",
       "      <td>2.711540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>746.993628</td>\n",
       "      <td>0.329892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.079181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>316.000000</td>\n",
       "      <td>2.499687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>514.000000</td>\n",
       "      <td>2.710963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>842.000000</td>\n",
       "      <td>2.925312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35991.000000</td>\n",
       "      <td>4.556194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Protein length      LogLength\n",
       "count   136193.000000  136193.000000\n",
       "mean       692.655775       2.711540\n",
       "std        746.993628       0.329892\n",
       "min         12.000000       1.079181\n",
       "25%        316.000000       2.499687\n",
       "50%        514.000000       2.710963\n",
       "75%        842.000000       2.925312\n",
       "max      35991.000000       4.556194"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_protein_lengths = protein_lengths.loc[protein_lengths['Common name'] == 'Human'].copy()\n",
    "# Note: without .copy(), some versions of Pandas may return a View.\n",
    "# This may interfere with adding a new column to human_protein_lengths.\n",
    "human_protein_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5dofE7aHSGpk",
    "outputId": "422cd712-ecdf-4f91-85e1-e0aeee1f405a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>1409012</td>\n",
       "      <td>Chotcza (2)</td>\n",
       "      <td>1511535.39</td>\n",
       "      <td>1890875.56</td>\n",
       "      <td>1920288.34</td>\n",
       "      <td>1852794.08</td>\n",
       "      <td>2123747.76</td>\n",
       "      <td>2649856.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0619042</td>\n",
       "      <td>Stary Brus (2)</td>\n",
       "      <td>1564085.98</td>\n",
       "      <td>1572661.84</td>\n",
       "      <td>1765578.55</td>\n",
       "      <td>1723233.19</td>\n",
       "      <td>2076285.49</td>\n",
       "      <td>2309878.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0602022</td>\n",
       "      <td>Aleksandrów (2)</td>\n",
       "      <td>1579356.44</td>\n",
       "      <td>1662725.31</td>\n",
       "      <td>2388276.59</td>\n",
       "      <td>2342380.81</td>\n",
       "      <td>2408083.72</td>\n",
       "      <td>2626930.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>1416092</td>\n",
       "      <td>Szulborze Wielkie (2)</td>\n",
       "      <td>1583373.09</td>\n",
       "      <td>1634649.25</td>\n",
       "      <td>1535709.13</td>\n",
       "      <td>1751927.47</td>\n",
       "      <td>2029410.65</td>\n",
       "      <td>2693226.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0613052</td>\n",
       "      <td>Podedwórze (2)</td>\n",
       "      <td>1587356.04</td>\n",
       "      <td>1784064.91</td>\n",
       "      <td>1724407.04</td>\n",
       "      <td>1681474.34</td>\n",
       "      <td>1959836.88</td>\n",
       "      <td>2896485.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>2612083</td>\n",
       "      <td>Szydłów (3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7753338.64</td>\n",
       "      <td>8139051.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>2817083</td>\n",
       "      <td>Wielbark (3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13103456.39</td>\n",
       "      <td>19412519.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>3006013</td>\n",
       "      <td>Jaraczewo (3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9296043.36</td>\n",
       "      <td>9682180.18</td>\n",
       "      <td>10663487.87</td>\n",
       "      <td>13167444.55</td>\n",
       "      <td>14740769.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>3007083</td>\n",
       "      <td>Opatówek (3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18666712.20</td>\n",
       "      <td>21464319.39</td>\n",
       "      <td>26876091.60</td>\n",
       "      <td>30935048.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>3209053</td>\n",
       "      <td>Mielno (3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36716532.07</td>\n",
       "      <td>37692325.45</td>\n",
       "      <td>41018680.08</td>\n",
       "      <td>41506722.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2509 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Code                 Region        2015        2016         2017  \\\n",
       "1040  1409012            Chotcza (2)  1511535.39  1890875.56   1920288.34   \n",
       "508   0619042         Stary Brus (2)  1564085.98  1572661.84   1765578.55   \n",
       "334   0602022        Aleksandrów (2)  1579356.44  1662725.31   2388276.59   \n",
       "1110  1416092  Szulborze Wielkie (2)  1583373.09  1634649.25   1535709.13   \n",
       "458   0613052         Podedwórze (2)  1587356.04  1784064.91   1724407.04   \n",
       "...       ...                    ...         ...         ...          ...   \n",
       "2041  2612083            Szydłów (3)         NaN         NaN          NaN   \n",
       "2157  2817083           Wielbark (3)         NaN         NaN          NaN   \n",
       "2202  3006013          Jaraczewo (3)         NaN  9296043.36   9682180.18   \n",
       "2214  3007083           Opatówek (3)         NaN         NaN  18666712.20   \n",
       "2449  3209053             Mielno (3)         NaN         NaN  36716532.07   \n",
       "\n",
       "             2018         2019         2020  \n",
       "1040   1852794.08   2123747.76   2649856.24  \n",
       "508    1723233.19   2076285.49   2309878.28  \n",
       "334    2342380.81   2408083.72   2626930.30  \n",
       "1110   1751927.47   2029410.65   2693226.02  \n",
       "458    1681474.34   1959836.88   2896485.76  \n",
       "...           ...          ...          ...  \n",
       "2041          NaN   7753338.64   8139051.84  \n",
       "2157          NaN  13103456.39  19412519.56  \n",
       "2202  10663487.87  13167444.55  14740769.18  \n",
       "2214  21464319.39  26876091.60  30935048.42  \n",
       "2449  37692325.45  41018680.08  41506722.97  \n",
       "\n",
       "[2509 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income = pd.read_csv('BDL municipality incomes 2015-2020.csv', sep=';', dtype={'Code': 'str'})\n",
    "income = income.sort_values(by='2015')\n",
    "income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3ckLNnwTrUp0"
   },
   "outputs": [],
   "source": [
    "voivodeship_names = {\n",
    "    '02': 'Dolnośląskie',\n",
    "    '04': 'Kujawsko-pomorskie',\n",
    "    '06': 'Lubelskie',\n",
    "    '08': 'Lubuskie',\n",
    "    '10': 'Łódzkie',\n",
    "    '12': 'Małopolskie',\n",
    "    '14': 'Mazowieckie',\n",
    "    '16': 'Opolskie',\n",
    "    '18': 'Podkarpackie',\n",
    "    '20': 'Podlaskie',\n",
    "    '22': 'Pomorskie',\n",
    "    '24': 'Śląskie',\n",
    "    '26': 'Świętokrzyskie',\n",
    "    '28': 'Warmińsko-mazurskie',\n",
    "    '30': 'Wielkopolskie',\n",
    "    '32': 'Zachodniopomorskie'\n",
    "}\n",
    "code_list = [s[:2] for s in income[\"Code\"]]\n",
    "name_list = [voivodeship_names[code] for code in code_list]\n",
    "income['Voivodeship'] = name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MtNNH8Xb0CoD",
    "outputId": "7a2b5a80-6ee5-4bcd-f127-f68d53727065"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>Voivodeship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0401031</td>\n",
       "      <td>Nieszawa (1)</td>\n",
       "      <td>2.996709e+06</td>\n",
       "      <td>3.657721e+06</td>\n",
       "      <td>6.470331e+06</td>\n",
       "      <td>4.252369e+06</td>\n",
       "      <td>3.861400e+06</td>\n",
       "      <td>5.492712e+06</td>\n",
       "      <td>Kujawsko-pomorskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>3024011</td>\n",
       "      <td>Obrzycko (1)</td>\n",
       "      <td>3.924365e+06</td>\n",
       "      <td>3.925119e+06</td>\n",
       "      <td>4.665922e+06</td>\n",
       "      <td>5.146760e+06</td>\n",
       "      <td>6.234464e+06</td>\n",
       "      <td>6.191140e+06</td>\n",
       "      <td>Wielkopolskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>3012011</td>\n",
       "      <td>Sulmierzyce (1)</td>\n",
       "      <td>4.088118e+06</td>\n",
       "      <td>4.694543e+06</td>\n",
       "      <td>4.790061e+06</td>\n",
       "      <td>5.132589e+06</td>\n",
       "      <td>5.426530e+06</td>\n",
       "      <td>5.670122e+06</td>\n",
       "      <td>Wielkopolskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0611021</td>\n",
       "      <td>Stoczek Łukowski (1)</td>\n",
       "      <td>4.178149e+06</td>\n",
       "      <td>4.137683e+06</td>\n",
       "      <td>4.045226e+06</td>\n",
       "      <td>4.431016e+06</td>\n",
       "      <td>6.169249e+06</td>\n",
       "      <td>6.908153e+06</td>\n",
       "      <td>Lubelskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>2213021</td>\n",
       "      <td>Skórcz (1)</td>\n",
       "      <td>5.234237e+06</td>\n",
       "      <td>6.662443e+06</td>\n",
       "      <td>6.685645e+06</td>\n",
       "      <td>7.402352e+06</td>\n",
       "      <td>7.656451e+06</td>\n",
       "      <td>1.070215e+07</td>\n",
       "      <td>Pomorskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>3064011</td>\n",
       "      <td>Poznań (1)</td>\n",
       "      <td>2.004614e+09</td>\n",
       "      <td>2.127838e+09</td>\n",
       "      <td>2.298017e+09</td>\n",
       "      <td>2.451093e+09</td>\n",
       "      <td>2.656515e+09</td>\n",
       "      <td>2.664275e+09</td>\n",
       "      <td>Wielkopolskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>1061011</td>\n",
       "      <td>Łódź (1)</td>\n",
       "      <td>2.386783e+09</td>\n",
       "      <td>2.473749e+09</td>\n",
       "      <td>2.536384e+09</td>\n",
       "      <td>2.627320e+09</td>\n",
       "      <td>2.829157e+09</td>\n",
       "      <td>2.895426e+09</td>\n",
       "      <td>Łódzkie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0264011</td>\n",
       "      <td>Wrocław (1)</td>\n",
       "      <td>2.697637e+09</td>\n",
       "      <td>2.840414e+09</td>\n",
       "      <td>2.969824e+09</td>\n",
       "      <td>3.174794e+09</td>\n",
       "      <td>3.353249e+09</td>\n",
       "      <td>3.295384e+09</td>\n",
       "      <td>Dolnośląskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>1261011</td>\n",
       "      <td>Kraków (1)</td>\n",
       "      <td>2.882882e+09</td>\n",
       "      <td>3.058201e+09</td>\n",
       "      <td>3.311715e+09</td>\n",
       "      <td>3.600811e+09</td>\n",
       "      <td>3.833362e+09</td>\n",
       "      <td>3.808958e+09</td>\n",
       "      <td>Małopolskie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>1465011</td>\n",
       "      <td>M.st.Warszawa od 2002 (1)</td>\n",
       "      <td>1.108331e+10</td>\n",
       "      <td>1.103323e+10</td>\n",
       "      <td>1.163487e+10</td>\n",
       "      <td>1.280553e+10</td>\n",
       "      <td>1.303188e+10</td>\n",
       "      <td>1.260886e+10</td>\n",
       "      <td>Mazowieckie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Code                     Region          2015          2016  \\\n",
       "172   0401031               Nieszawa (1)  2.996709e+06  3.657721e+06   \n",
       "2341  3024011               Obrzycko (1)  3.924365e+06  3.925119e+06   \n",
       "2255  3012011            Sulmierzyce (1)  4.088118e+06  4.694543e+06   \n",
       "436   0611021       Stoczek Łukowski (1)  4.178149e+06  4.137683e+06   \n",
       "1731  2213021                 Skórcz (1)  5.234237e+06  6.662443e+06   \n",
       "...       ...                        ...           ...           ...   \n",
       "2393  3064011                 Poznań (1)  2.004614e+09  2.127838e+09   \n",
       "791   1061011                   Łódź (1)  2.386783e+09  2.473749e+09   \n",
       "168   0264011                Wrocław (1)  2.697637e+09  2.840414e+09   \n",
       "974   1261011                 Kraków (1)  2.882882e+09  3.058201e+09   \n",
       "1293  1465011  M.st.Warszawa od 2002 (1)  1.108331e+10  1.103323e+10   \n",
       "\n",
       "              2017          2018          2019          2020  \\\n",
       "172   6.470331e+06  4.252369e+06  3.861400e+06  5.492712e+06   \n",
       "2341  4.665922e+06  5.146760e+06  6.234464e+06  6.191140e+06   \n",
       "2255  4.790061e+06  5.132589e+06  5.426530e+06  5.670122e+06   \n",
       "436   4.045226e+06  4.431016e+06  6.169249e+06  6.908153e+06   \n",
       "1731  6.685645e+06  7.402352e+06  7.656451e+06  1.070215e+07   \n",
       "...            ...           ...           ...           ...   \n",
       "2393  2.298017e+09  2.451093e+09  2.656515e+09  2.664275e+09   \n",
       "791   2.536384e+09  2.627320e+09  2.829157e+09  2.895426e+09   \n",
       "168   2.969824e+09  3.174794e+09  3.353249e+09  3.295384e+09   \n",
       "974   3.311715e+09  3.600811e+09  3.833362e+09  3.808958e+09   \n",
       "1293  1.163487e+10  1.280553e+10  1.303188e+10  1.260886e+10   \n",
       "\n",
       "             Voivodeship  \n",
       "172   Kujawsko-pomorskie  \n",
       "2341       Wielkopolskie  \n",
       "2255       Wielkopolskie  \n",
       "436            Lubelskie  \n",
       "1731           Pomorskie  \n",
       "...                  ...  \n",
       "2393       Wielkopolskie  \n",
       "791              Łódzkie  \n",
       "168         Dolnośląskie  \n",
       "974          Małopolskie  \n",
       "1293         Mazowieckie  \n",
       "\n",
       "[301 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_a_city = [s[-1] == '1' for s in income['Code']]\n",
    "city_income = income[is_a_city].dropna()\n",
    "city_income = city_income[city_income.apply(lambda x: all(x!=0), axis=1)]  # removing zero incomes\n",
    "city_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOgHV-dWr3rm"
   },
   "source": [
    "## Estimating the test error of regression with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrPhbsbjqZib"
   },
   "source": [
    "Cross validation is a technique to estimate the test error of a prediction algorithm (either a classifier or a regression).  \n",
    "The idea is to divide the training data set into $k$ parts and, for each part, use it as the test set and the rest as the training set.  \n",
    "We will see how well this approach approximates a true test error and what happens for different values of $k$.  \n",
    "\n",
    "To evaluate models, we typically partition our data set into three subsets: a *training set* on which we train our models, a *validation set* on which we test our models to select the best one, and a *test set* on which we do a final evaluation of our model and e.g. compare it to a baseline model. One of the approaches to do cross validation is to hold out a portion of the data set as the test set and to split the remaining data into a series of validation sets and training sets.\n",
    "\n",
    "You should be aware that sometimes people reverse the meaning of the *validation* and the *test* sets. This is the case in the picture below, which is taken from Wikipedia - it depicts a series of test sets rather than validation sets. Although the terminology is inconsistent, the general idea stays the same, so if you have a good understanding of how cross validation works, you won't be confused.\n",
    "\n",
    "\n",
    "<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/K-fold_cross_validation_EN.svg/1920px-K-fold_cross_validation_EN.svg.png'>\n",
    "\n",
    "*A diagram representing the k-fold cross validation.       \n",
    "Source: https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg*\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2GdZaHuSNGs"
   },
   "source": [
    "**Exercise 1.** In this exercise, we will build a simple linear regression model to predict the income of cities in Poland in the year 2017 based on the incomes in 2015 and 2016, and we will check how well the k-fold cross validation approximates its test error. We will use the data in the `city_income` data frame generated in the *Data & modules* section. Let $R$ be the number of rows in this data frame.   \n",
    "\n",
    "1. Perform a log-transformation of the incomes.  \n",
    "2. Permute the rows of the data frame randomly in order to eliminate any grouping of the data that could bias the estimation of the errors. You can use the `city_income.sample(frac=1)` command.  \n",
    "3. Create a data frame with the first $T=50$ observations. This will be our *test set*, where our model will need to predict the incomes. The RMSE of prediction on this set is called the *test error*.  \n",
    "4. Create a data frame with the remaining $R-T$ observations. This will be our *full training set*, where our model will learn how to make predictions. The RMSE of prediction on this set is called the *training error.*  \n",
    "5. Using the full training data set, create a linear regression model to predict the log-incomes in 2017 based on the log-incomes in 2015 and 2016.\n",
    "Predict the log-incomes in 2017 for the test set and calculate the true value of the test error.  \n",
    "6. Now, we will try to use k-fold cross validation to predict the test error using only the training data set (i.e. without looking at the test set at all). Create a list with several values of the $k$ parameter, including 5, 10 (the two most commonly used values), 15, 60, and the size of the full training set (for a so-called *leave-one-out* cross validation).  \n",
    "7. For each value of $k$, perform a $k$-fold cross-validation. For $i=1,\\dots, k$:  \n",
    "  7.1. Select observations $(i-1)\\lfloor (R-T)/k \\rfloor$ up to $i\\lfloor (R-T)/k \\rfloor$ from the full training data set and save them as the *validation set*. Save the remaining observations as a *partial training set*.  \n",
    "  7.2. Create a linear regression model on the partial training set. Calculate its predictions for the validation set. Calculate the validation error (RMSE) and store it in a list.  \n",
    "8. For each $k$, calculate the mean and standard deviation of the corresponding validation errors.   \n",
    "  8.1. Can you see some trends in those values? Can you explain it?\n",
    "  8.2. Which $k$ best approximates the true test error?\n",
    "9. Plot the distributions of errors on histograms.   \n",
    "  9.1. Which $k$ gives you the most information about the distribution of the test error?     \n",
    "10. Check what would happen if we didn't do the log-transformation in step 1. Can you explain the results?  \n",
    "  10.1.\\* To give a more detailed answer, do an exploratory analysis and figure out the distribution of the incomes.   \n",
    "11. Check what would happen if we didn't do the permutation in step 2. Can you explain the results?\n",
    "12. Let's suppose we don't know the incomes of the cities for the year 2018. Can we use our model to predict them? Can we estimate the prediction error using k-fold cross validation? Why/why not?    \n",
    "13. In point 7, we have iterated over $i=1,\\dots, k$. Do you see a potential problem or inefficiency when $k$ doesn't divide $R-T$? Does it make sense to iterate over $i=1,\\dots, k+1$? What are the possible advantages and disadvantages?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "city_income[[\"2015\", \"2016\", \"2017\"]] = city_income[[\"2015\", \"2016\", \"2017\"]].apply(lambda x: np.log(x))\n",
    "\n",
    "city_income = city_income.sample(frac=1)\n",
    "\n",
    "test_set = city_income.iloc[:50]\n",
    "\n",
    "train_set = city_income.iloc[50:]\n",
    "\n",
    "X_train = train_set[[\"2015\", \"2016\"]]\n",
    "y_train = train_set[\"2017\"]\n",
    "model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error for k=5: 0.07546002718233172\n",
      "Train error for k=5: 0.074473557358538\n",
      "Test error for k=10: 0.07357578185909631\n",
      "Train error for k=10: 0.07463944349135318\n",
      "Test error for k=15: 0.07126945408450105\n",
      "Train error for k=15: 0.07467424153155987\n",
      "Test error for k=60: 0.06409409091345158\n",
      "Train error for k=60: 0.07477095947588579\n",
      "Test error for k=251: 0.050622306050507423\n",
      "Train error for k=251: 0.07479391702216164\n"
     ]
    }
   ],
   "source": [
    "X_test = test_set[[\"2015\", \"2016\"]]\n",
    "y_test = test_set[\"2017\"]\n",
    "y_pred = model.predict(X_test)\n",
    "test_error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "k_values = [5, 10, 15, 60, len(train_set)]\n",
    "for k in k_values:\n",
    "    kf = KFold(n_splits=k)\n",
    "    errors = []\n",
    "    errors_train = []\n",
    "    for train_index, val_index in kf.split(train_set):\n",
    "        X_train_k, X_val_k = train_set.iloc[train_index][[\"2015\", \"2016\"]], train_set.iloc[val_index][[\"2015\", \"2016\"]]\n",
    "        y_train_k, y_val_k = train_set.iloc[train_index][\"2017\"], train_set.iloc[val_index][\"2017\"]\n",
    "        model_k = LinearRegression().fit(X_train_k, y_train_k)\n",
    "        y_pred_k = model_k.predict(X_val_k)\n",
    "        errors.append(np.sqrt(mean_squared_error(y_val_k, y_pred_k)))\n",
    "        Y_train_pred = model_k.predict(X_train_k)\n",
    "        errors_train.append(np.sqrt(mean_squared_error(y_train_k, Y_train_pred)))\n",
    "    print(f\"Test error for k={k}: {sum(errors)/len(errors)}\")\n",
    "    print(f\"Train error for k={k}: {sum(errors_train)/len(errors_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFWT4ZUn0DJd"
   },
   "source": [
    "**Exercise 2.** Let's make a better model! We can reasonably assume that the average income is different in each voivodeship, rather than common for the whole country. Next, we can assume a more complex relation between the incomes - for example, the income in a given year may depend on the previous incomes, but also on the squares of the previous incomes, and the products of the previous incomes. This corresponds to a model with an interaction - a high income in both 2015 and 2016 is a better indicator of a high income in 2017 than a high income in just a single year. This is a reasonable assumption, because consistently stable incomes indicate a stable economy, while a high income in just a single year can be caused by random events. Both assumptions can improve our model by making it fit the data better. What could possibly go wrong?  \n",
    "\n",
    "1. Write down the mathematical equation for the model described above. Are you going to use a model with an intercept?   \n",
    "2. Create a data frame `X` with the independent variables: the city incomes in 2015 and 2016, their second powers, their product, and voivodeships encoded as dummy variables. Use the `pd.get_dummies` variables to encode the voivodeships. Create a data frame `Y` with the independent variable (the incomes in 2017).  \n",
    "3. Withhold $T=50$ observations from `X` and `Y` as the *test* data set, and remove these observations from `X` and `Y`.  \n",
    "4. Create a linear regression model on the *training* data set. Calculate its train and test RMSE. Compare the errors with the model from Exercise 1.  \n",
    "  4.1. Did the train RMSE decrease? Why?  \n",
    "  4.2. Did the test RMSE decrease as well? Why?  \n",
    "  4.3. Is the test RMSE considerably higher than the train RMSE? Is the difference between those errors larger than in exercise 1?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#city_income = city_income.sample(frac=1)\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['X_2015'] = city_income['2015']\n",
    "X['X_2016'] = city_income['2016']\n",
    "X['X_2015^2'] = city_income['2015'] ** 2\n",
    "X['X_2016^2'] = city_income['2016'] ** 2\n",
    "X['X_2015*X_2016'] = city_income['2015'] * city_income['2016']\n",
    "X = pd.concat([X, pd.get_dummies(city_income['Voivodeship'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = city_income['2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "\n",
    "X_train = X[:-T]\n",
    "X_test = X[-T:]\n",
    "Y_train = Y[:-T]\n",
    "Y_test = Y[-T:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.07\n",
      "Test RMSE: 0.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.2f}')\n",
    "print(f'Test RMSE: {test_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lujJfiRD24py"
   },
   "source": [
    "**Exercise 3.** Even if a model is reasonable, it may perform poorly if it's too flexible and overfits to the data. In the previous exercise, you may, or may not have, concluded that the new model overfits. However, any result obtained that way is not very robust - after all, we have checked the performance only on a single selected test set. Maybe the result was just a random artifact caused by an unfortunate permutation of the rows of `city_income`? That's where cross-validation again comes in handy.    \n",
    "\n",
    "\n",
    "1. Create a data frame `X` containing the independent variables (predictors) like in Exercise 2. Don't withhold the test set this time - we won't need it anymore.  \n",
    "2. Select your favorite value of $k$ for a k-fold cross validation.  \n",
    "3. Use the `cross_val_score` function from the `sklearn` library to evaluate the complex model from Exercise 2 and the simple model from Exercise 1. To evaluate a model with RMSE, use a keyword argument `scoring='neg_root_mean_squared_error'`.   \n",
    "  3.1. Compare the average test errors of the two models. Can you conclude that the complex model overfits?  \n",
    "  3.2. Check the standard deviation of the errors. Is it much lower than the average of the errors? Is the conclusion about overfitting robust, or could it occur just by chance?      \n",
    "  3.3. \\* Can you come up with some statistical method to decide whether the test RMSE of the complex model really is higher than the RMSE of the simple model, rather than being caused just by random chance?\n",
    "4. \\* Implement a k-fold cross validation and, in each iteration, save *both* the training and the test RMSE of the fitted model (or figure out how to get the train RMSE with `sklearn`).   \n",
    "  4.1 Calculate the mean and standard deviation of the four errors. Analyze the difference between the test MSE and the train MSE of the two models. Does this provide some additional useful information compared to just the test MSE?  \n",
    "5. Can the conclusion about overfitting depend on the value of $k$? Why/why not?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['2015'] = city_income['2015']\n",
    "X['2016'] = city_income['2016']\n",
    "X['2015^2'] = city_income['2015'] ** 2\n",
    "X['2016^2'] = city_income['2016'] ** 2\n",
    "X['2015*2016'] = city_income['2015'] * city_income['2016']\n",
    "X = pd.concat([X, pd.get_dummies(city_income['Voivodeship'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test error for the model 1: 0.0778398585237707\n",
      "Standard deviation of the test errors for the model 1: 0.02103187794847424\n",
      "Average test error for the model 2: 0.07695327148774601\n",
      "Standard deviation of the test errors for the model 2: 0.021989585324403375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "y = city_income[\"2017\"]\n",
    "\n",
    "model1 = LinearRegression().fit(X, y)\n",
    "\n",
    "scores1 = -cross_val_score(model1, X, y, cv=k, scoring=\"neg_root_mean_squared_error\")\n",
    "print(f\"Average test error for the model 1: {np.mean(scores1)}\")\n",
    "print(f\"Standard deviation of the test errors for the model 1: {np.std(scores1)}\")\n",
    "\n",
    "model2 = LinearRegression().fit(X[[\"2015\", \"2016\"]], y)\n",
    "\n",
    "scores2 = -cross_val_score(model2, X[[\"2015\", \"2016\"]], y, cv=k, scoring=\"neg_root_mean_squared_error\")\n",
    "print(f\"Average test error for the model 2: {np.mean(scores2)}\")\n",
    "print(f\"Standard deviation of the test errors for the model 2: {np.std(scores2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train RMSE 1: 0.07265422426298276\n",
      "Mean Test RMSE 1: 0.05299344191395331\n",
      "Standard Deviation of train RMSE 1: 0.0005419213427168018\n",
      "Standard Deviation of Test RMSE 1: 0.06002901714545975\n",
      "Mean Train RMSE 2: 0.07834784912289537\n",
      "Mean Test RMSE 2: 0.051426546095853516\n",
      "Standard Deviation of train RMSE 2: 0.0005353992703766085\n",
      "Standard Deviation of Test RMSE 2: 0.06162237905856489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def do_LR(X_train, X_test, y_train, y_test):\n",
    "    model1_cross = LinearRegression().fit(X_train, y_train)\n",
    "    y_train_pred1 = model1_cross.predict(X_train)\n",
    "    y_test_pred1 = model1_cross.predict(X_test)\n",
    "\n",
    "    train_rmse1 = rmse(y_train, y_train_pred1)\n",
    "    test_rmse1 = rmse(y_test, y_test_pred1)\n",
    "    return train_rmse1, test_rmse1\n",
    "\n",
    "train_errors1 = []\n",
    "test_errors1 = []\n",
    "train_errors2 = []\n",
    "test_errors2 = []\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=300)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    train_rmse1, test_rmse1 = do_LR(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index][[\"2015\", \"2016\"]], X.iloc[test_index][[\"2015\", \"2016\"]]\n",
    "    \n",
    "    train_rmse2, test_rmse2 = do_LR(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    train_errors1.append(train_rmse1)\n",
    "    test_errors1.append(test_rmse1)\n",
    "    \n",
    "    train_errors2.append(train_rmse2)\n",
    "    test_errors2.append(test_rmse2)\n",
    "\n",
    "print(f\"Mean Train RMSE 1: {np.mean(train_errors1)}\")\n",
    "print(f\"Mean Test RMSE 1: {np.mean(test_errors1)}\")\n",
    "print(f\"Standard Deviation of train RMSE 1: {np.std(train_errors1)}\")\n",
    "print(f\"Standard Deviation of Test RMSE 1: {np.std(test_errors1)}\")\n",
    "\n",
    "print(f\"Mean Train RMSE 2: {np.mean(train_errors2)}\")\n",
    "print(f\"Mean Test RMSE 2: {np.mean(test_errors2)}\")\n",
    "print(f\"Standard Deviation of train RMSE 2: {np.std(train_errors2)}\")\n",
    "print(f\"Standard Deviation of Test RMSE 2: {np.std(test_errors2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test - train MSE for the model 1: 0.019660782349029453\n",
      "test - train MSE for the model 2: 0.026921303027041855\n"
     ]
    }
   ],
   "source": [
    "print(f\"test - train MSE for the model 1: {np.mean(train_errors1) - np.mean(test_errors1)}\")\n",
    "print(f\"test - train MSE for the model 2: {np.mean(train_errors2) - np.mean(test_errors2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4PYWS9CRvKI"
   },
   "source": [
    "## Analyzing estimators with the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXweAU-UR7F7"
   },
   "source": [
    "The Bootstrap is a non-parametric technique used to quantify the uncertainty of estimation and to perform hypothesis testing.   \n",
    "As you may remember, *non-parametric* means that you don't need to know the probability distribution of your data to use this technique.  \n",
    "Therefore, Bootstrap allows you to test hypotheses for any data sets, not just normally distributed ones. However, it has the same downsides as all the other non-parametric techniques.  \n",
    "In particular, it's difficult to analyze theoretically, so we can't really say how well it performs by analyzing it like in the case of the Student's t-test.  \n",
    "\n",
    "The idea behind bootstrap is as follows. Suppose we have a statistical sample of size $n$, $X_1, \\dots, X_n$, and some statistic (e.g. the mean or the variance) that we compute from this sample, denoted $f(X_1, \\dots, X_n)$, to estimate some parameter.  \n",
    "Suppose we want to quantify the uncertainty of this estimation, but don't know how to calculate the variance of $f(X_1, \\dots, X_n)$ analytically.  \n",
    "In this case, we can sample $n$ observations **with replacement** from $X_1, \\dots, X_n$ to get a new sample $X_1^*, \\dots, X_n^*$, calculate $f(X_1^*, \\dots, X_n^*)$, and repeat this procedure multiple times to get a sample of values of the statistic $f$.  \n",
    "It turns out that the distribution of $f$ obtained this way mimics its true distribution.   \n",
    "\n",
    "It's important to remember that to get a proper result, we need to get a  sample of the same size (otherwise, we change the parameters of our statistic - e.g. a mean computed from a smaller sample will have a higher variance), and to sample with replacement (otherwise, if we sample without replacement, we just get a permutation of the values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJWkGRhra8lL"
   },
   "source": [
    "**Exercise 4.** In this exercise, we will use the bootstrap technique to analyze the properties of the estimator of the standard deviation of a protein log-length. We will use this to decide which type of a confidence interval for the mean we can use.  \n",
    "\n",
    "Just to remind you, there are three common types of a confidence interval for the mean of a normally distributed population. The first type is for a known $\\sigma$ and is given by\n",
    "$$\\left (\\hat{\\mu} - q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}},\\quad \\hat{\\mu} + q_{(1+\\alpha)/2}\\frac{\\sigma}{\\sqrt{N}} \\right ), $$\n",
    "and the second type, for an unknown $\\sigma$, is given by\n",
    "$$\\left (\\hat{\\mu} - t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}},\\quad \\hat{\\mu} + t_{(1+\\alpha)/2, N-1}\\frac{\\hat{\\sigma}}{\\sqrt{N}} \\right )$$\n",
    "The third type, called an *asymptotic* confidence interval, is given by the same formula as the first one, but with estimated standard deviation (i.e. put $\\hat{\\sigma}$ instead of $\\sigma$ in the equation). If we can estimate $\\sigma$ *very precisely*, we may choose to use this third type to obtain a more precise result (i.e. a shorter interval).   \n",
    "\n",
    "1. Select $N=50$ random samples from the `human_protein_lengths` data and put them into a new data frame called e.g. `sample`. Calculate the standard deviation of the sampled protein log-lengths.  \n",
    "2. Now, we will use the bootstrap to estimate the distribution of the estimator. Repeat the following $R=10 000$ times:  \n",
    "  2.1. Resample the observations from the `sample` data frame, e.g. using the `sample.sample()` method.  \n",
    "  2.2. Calculate the standard deviation from the resampled observations and store it in a list.   \n",
    "3. Plot the obtained bootstrap replicates of the standard deviation on a histogram. Calculate the mean and standard deviation of the estimator, and check its coefficient of variation.\n",
    "  3.1. In your opinion, is the standard deviation estimated sufficiently precisely to use the asymptotic confidence interval?  \n",
    "  3.2. How does the error in the estimation of the standard deviation influence the length of the asymptotic confidence interval?   \n",
    "  3.3. Create a histogram showing bootstrapped lengths of the asymptotic confidence interval. To calculate the lengths, you can use the bootstraped replicates of the standard deviation.   \n",
    "  3.4. Do the answers to the previous points depend on $N$?  \n",
    "4. Estimate the true distribution of the estimator of the standard deviation. To do this, draw $R$ samples of size $N$ of the log-lengths of human proteins and calculate the standard deviation for each sample.  \n",
    "  4.1. Plot the obtained $R$ values of the estimator on a histogram and compare it to the histogram of bootstrapped replicates. Are the distributions similar?  \n",
    "  4.2. Use the $R$ values to estimate the expected value and standard deviation of the estimator. Compare it to the values obtained with the bootstrap. Are the bootstrap estimates similar to the true parameters of this estimator?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "N = 50\n",
    "sample = human_protein_lengths.sample(n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log_lengths = np.log(sample['LogLength'])\n",
    "sample_std = np.std(sample_log_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = 10000\n",
    "bootstrap_std = []\n",
    "for i in range(R):\n",
    "    resampled = sample_log_lengths.sample(n=N, replace=True)\n",
    "    bootstrap_std.append(np.std(resampled))\n",
    "\n",
    "plt.hist(bootstrap_std, bins=30)\n",
    "plt.title('Bootstrap Replicates of Standard Deviation')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.14, Standard Deviation: 0.02, Coefficient of Variation: 0.13\n"
     ]
    }
   ],
   "source": [
    "bootstrap_mean = np.mean(bootstrap_std)\n",
    "bootstrap_std = np.std(bootstrap_std)\n",
    "cv = bootstrap_std / bootstrap_mean\n",
    "print(f\"Mean: {bootstrap_mean:.2f}, Standard Deviation: {bootstrap_std:.2f}, Coefficient of Variation: {cv:.2f}\")\n",
    "# a good estimate of the true standard deviation of the population. Therefore, it can be used to calculate the asymptotic confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAddElEQVR4nO3debgdVZnv8e8vkxCIMkUgE0GJSpSAGMCBFlDpJojG4SogINAC0orIFYWIXuRK0+i1tdELGCJwmRSUQQwYBCeg+2oagiISEI0ESAhDgkDC0ISEt/9Y6xSVzR7qnJw6Oyf8Ps9znrOrau2qd1WtqnfXqtq1FRGYmZkBDOl2AGZmtu5wUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KaznJN0n6T3djqOdgYhR0paSbpa0QtI361zWukzSdZIO7XYcZY3bRtJJks5tU36db9PrqirrbtAkhVyZZyU9JelxST+VNL4f5jtRUkgaVrF8SNpubZe7LpB0gaR/Xt+XmR0FLANeGRHHtyok6ZS8jXcduNCqybFdsjblI2JaRFzYh2VL0rGS7pT0tKTFki6XtENv59XEGtsmIv4lIo7oh/kOCEk3SqoUbxfbf2WDJilk74uIjYGtgUeA/9vleF6ianKxAbcNcFe0+bamJAGHAH8D1qlP0+uAbwOfBY4FNgNeB1wNvLcf5t1x21gyIMeXiBgUf8B9wHtKw/sCfy4Nvwq4CFgK3A98GRiSpw3Jw/cDj+Zyr8rTHgACeCr/vQ3YDrgJeJL0CeaHuezNuezTuez+wJ7AYuBE4GHgYmBT4Nocy+P59bhSrDcCpwO35GX8BNgsT5uYl3EUsAR4CDi+9N4hwAzgr8BjwI963punH5Lr+Rjwpcb11rBOLwD+ucW0/YDbgSeA3wBTGrbF54E7cvw/BDYoTT8hx70EOCLXZ7tcp+eBlXn9XdNpfsAWef09QTpY/3vPdm0S89uBW/M8bgXeXqpnebmt1sc7gWeBg/P6G1Ga1qpNnAV8s2E+1wDHler2hVy3p4HzgC2B64AVwC+ATTtte2CfHP/zuQ5/yOPHALPzulkAHNmh/I3AEaVYjwTuzrHcBezcZL1MAlYDu7bZP9vtf4cB/wH8K2l/WAhMa7VtgFOAS6q0adrsD6X1eShpP18GfKk036HASfm9K4DbgPF52huAn+f1eg/w0TZ1L9YpLx4Pjicdax4CDs/TWrX/McCVed0tBI4tzfsU4ArgEmA5cDKpjZb3+Tfnug0HXgv8Kq+LZcD3gU1aHUeb1qebB/re/DU0hJHAhcBFpekXkQ6uo3Jj+DPwiTztH0k7zGuAjYGrgIsbGs6w0rwuzY1vCLABsHtpWgDblYb3BFYBXwdeAWwIbA58OMc5CrgcuLqhET0IvAnYKDeISxriuTRP2yE3lp66HwfMBcbl5Z0DXJqnTc6N7Z152rdybL1KCsDOuUHvRtpxDs3r/xWlbXFLbsybkQ4qR5cORg8Db8z1v7i8zpots8P8Tgdmkhr8cODvADWJeTPSAecQYBhwYB7evF1dG+ZxHumgMpy0U32oU5sAdiUdwHsOgFsAzwBbluo2l5QIxub1+jvSjvwK0g78lYrb/hRKB8s87ibg7BzTTrn8u9uUv5EXD2AfIbXDXQCREt82TdbL0cD9HdZdu/3vMNLB8EhSe/qnvM7UbNuU46ZDm6b9/tCzPr9H2i93BJ4Dts/TvwD8EXh9rv+OpH13I2ARcDipLe1MOsC+sUXdy+t0zxzfV0ntaN/cHjZtUdchpGR0MjCCdIy6F/iH0rp4HvhALrshqc0cWZrHN4CZ+fV2wN55XYwmfZA9o9lxtOW2HKiD+tr+5co8RfrEuCo3qh3ytKF5Y08ulf8kcGN+/UvgU6Vpr88rehjNk8JFwCxKn+5L05olhZWUPik3ec9OwOMNjehrpeHJeR5DS/G8oTT9/wDn5dd3k3f6PLx1qS4nA5eVpm2U59vbpPBd4NSGcfcAe5S2xcEN8fU0yvOB00vTtqNaUmg1v6+SDjbbNatD6T2HALc0jPstcFi7upbKjiR9EvtAHj4H+EnFNnE3sHd+fQwwp6FuB5WGrwS+Wxr+DPkDQ4VtfwprfoIeT/oEP6o07nTggmblS22v5wB2PfDZCvvel4C5baZ32v8OAxY0rOsAtmq2bVgzKbRt07TfH3rWZ/ks/RbggFKbnt6kPvsD/94w7hxy8m5SvrxO9yR9ki8fTx4F3tqirrsBDzTM74vA/yuti5sbph8B/Cq/FimBvbNFbB8Aft/QHtsmhcF2TeEDEbEJKQseA9wkaSvSp7MRpFPMHveTPplB+gTaOG0Y6dNbMyeQVvYtkuZL+scOcS2NiP/qGZA0UtI5ku6XtJyUrTeRNLT0nkUN8QzP9Wg1fUx+vQ3wY0lPSHqCtFOsznUZU35fRDxN+sTbW9sAx/csIy9nfCkGSGcDPZ4hnYHRGEPD63Zaze8bpLO8GyTdK2lGi/c3bmNYsw108kHSh405efj7wDRJo/NwuzZxIanLifz/4oZ5P1J6/WyT4Y3XLN5y2zcaA/wtIlY0lK9a5/GkrpNOHiMdbFvptP9BaftGxDP5ZWO9m+nUptvtDy9ZNmu2rVb13wbYraH9HwRsVSFegMciYlWLZTZb1piGZZ3UEH/jPnQF8DZJY0hnUEHqVkXSqyVdJunBfOy5hDWPKx0NtqQAQESsjoirSBt/d9Kp3fOkFdxjAunUGNJZReO0VaSdM5rM/+GIODIixpA+8Zzd4Y6jxnkcTzob2S0iXknacJAOKj3Kd05NyPEvazN9SX69iNQfu0npb4OIeJDUf1m8T9JI0ulwby0CTmtYxsiIuLTCex8inco3qwc0Wd/tRMSKSHekvAZ4H/A5Se9uUrRxG8OabaCTQ0k77gOSHiZ1+Q0ndUN1ahOXANMl7QhsT7oAuzZabfvGdbcE2EzSqIbyD7Yo32gRqQ+6k18C4yRNbTG90/63Njq16Xb7Qyet6r8IuKlhnhtHxD+tTUWyxm2yCFjYsKxREbFvq/dExBPADcBHgY+Rust6ypyey0/Jx56DWfO409GgTAr59rjppAu6d0fEalJf8GmSRknaBvgcaWeF1Ef7PyVtK2lj4F9IFwpXkfpgXyD15fXM/yOSeg5sj5NW8uo8/Ei5bAujSJ8An5C0GfCVJmUOljQ5N/KvAlfkevT4X/mM442kvs0f5vEzcz23ybGOzusC0ieI/STtLmlEnm+nbTxU0galvxGkPtijJe2W1/VGkt7bcPBp5UfA4ZK2z3U7uWF6lfVXkLSfpO3ynUHLSdthdZOic4DXSfqYpGGS9id1y11bYRljgXeTLq7vlP92JF0nOjSXadkmImIx6cL2xcCVEfFs1fq10GrbPwJMlDQkL3cR6SaA0/O2mwJ8gnSW85LyTZwLfF7SW/J23q6nXZVFxF9I1y0ulbSnpBF5eQdImlFh/1sbndp0u/2hk3OBUyVNyvWfImlzUpt5naRDJA3Pf7tI2r4f6tPY/m8Blks6UdKGkoZKepOkXTrM5wfAx0nXLn9QGj+K3M2e2/UXehvgYEsK10h6inRwOA04NCLm52mfId3dcS/pTocfkPq3yf8vJnXjLAT+K5fvOZU9Dfj/+fTtraQLb/+ZlzWb1O+6MM/rFODCXPajLeI8g3RBaBnpItjPmpS5mNS/+DDpIuGxDdNvInWb/BL414i4IY//do7pBkkr8vx3y3WZD3w61/0h0sFrcYsYe8wgJbCev19FxDzSRcEz8zwWkPqFO4qI64DvAL/O7/ttnvRc/n8eMDmvv6srzHIS6Q6dp/K8zo6IG5ss9zHSQf14UvfCCcB+EbGssWwThwC3R8QN+Yzg4Yh4ONdjiqQ30b5NQOpC2oGXdh31Rattf3n+/5ik3+XXB5L6zpcAPyb1e/+8TflCRFxOavs/IN19czXpgn0zx5Law1mk63p/JXW5XZOnt9v/+qxCm265P1TwLVIyu4F0TDkP2DB3x/09cABpvT7MizeSrK012n9OqO8jfRBZSDpmnEu6m6ud2aR945GI+ENp/P8mXRh/Evgp6aaaXum5+m8DSNKNpAtpL/nWpqSJpMYxvKFfclDKn67uJN25NOjr04qkd5I+GU+MiBf6OI+JrEfb3ganwXamYIOApA/mLoZNSZ+wrlmfD3KShpO+2HVuXxOC2brCScHq8EnStZq/kvrd++MC3Topnwk9Qbo754yuBmPWD9x9ZGZmBZ8pmJlZYdA9vG2LLbaIiRMndjsMM7NB5bbbblsWEaM7lRt0SWHixInMmzev22GYmQ0qkhq/8d+Uu4/MzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlaoLSlIOl/So5LubDFdkr4jaYGkOyTtXFcsZmZWTZ1nCheQfq+3lWmkR79OIv2g9XdrjMXMzCqoLSlExM3A39oUmQ5cFMlc0s9VtvvJPzMzq1k3v9E8ljV/e3RxHvdQY0FJR5HOJpgwYcKABGfWFxNn/LRSufu+9t6aIzHrm25eaG72u6FNH9kaEbMiYmpETB09uuOjO8zMrI+6mRQWs+YPlI/jxR8oNzOzLuhmUpgNfDzfhfRW4MmIeEnXkZmZDZzarilIuhTYE9hC0mLgK8BwgIiYCcwB9iX9QPkzwOF1xWJmZtXUlhQi4sAO0wP4dF3LNzOz3vM3ms3MrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVak0KkvaRdI+kBZJmNJn+KknXSPqDpPmSDq8zHjMza6+2pCBpKHAWMA2YDBwoaXJDsU8Dd0XEjsCewDcljagrJjMza6/OM4VdgQURcW9ErAQuA6Y3lAlglCQBGwN/A1bVGJOZmbVRZ1IYCywqDS/O48rOBLYHlgB/BD4bES80zkjSUZLmSZq3dOnSuuI1M3vZqzMpqMm4aBj+B+B2YAywE3CmpFe+5E0RsyJiakRMHT16dH/HaWZmWZ1JYTEwvjQ8jnRGUHY4cFUkC4CFwBtqjMnMzNqoMyncCkyStG2+eHwAMLuhzAPAuwEkbQm8Hri3xpjMzKyNYXXNOCJWSToGuB4YCpwfEfMlHZ2nzwROBS6Q9EdSd9OJEbGsrpjMzKy92pICQETMAeY0jJtZer0E+Ps6YzAzs+r8jWYzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWqJQUJL2p7kDMzKz7qp4pzJR0i6RPSdqkzoDMzKx7KiWFiNgdOAgYD8yT9ANJe9camZmZDbjK1xQi4i/Al4ETgT2A70j6k6QP1RWcmZkNrKrXFKZI+jfgbuBdwPsiYvv8+t/avG8fSfdIWiBpRosye0q6XdJ8STf1oQ5mZtZPhlUsdybwPeCkiHi2Z2RELJH05WZvkDQUOAvYG1gM3CppdkTcVSqzCXA2sE9EPCDp1X2rhpmZ9YeqSWFf4NmIWA0gaQiwQUQ8ExEXt3jPrsCCiLg3v+cyYDpwV6nMx4CrIuIBgIh4tA91MDOzflL1msIvgA1LwyPzuHbGAotKw4vzuLLXAZtKulHSbZI+XjEeMzOrQdUzhQ0i4qmegYh4StLIDu9Rk3HRZPlvAd5NSjq/lTQ3Iv68xoyko4CjACZMmFAxZDMz662qZwpPS9q5Z0DSW4Bn25SHdGYwvjQ8DljSpMzPIuLpiFgG3Azs2DijiJgVEVMjYuro0aMrhmxmZr1V9UzhOOByST0H9a2B/Tu851ZgkqRtgQeBA0jXEMp+ApwpaRgwAtiNNnczmZlZvSolhYi4VdIbgNeTuoX+FBHPd3jPKknHANcDQ4HzI2K+pKPz9JkRcbeknwF3AC8A50bEnWtRHzMzWwtVzxQAdgEm5ve8WRIRcVG7N0TEHGBOw7iZDcPfAL7RizjMzKwmlZKCpIuB1wK3A6vz6ADaJgUzMxtcqp4pTAUmR0Tj3UNmZrYeqXr30Z3AVnUGYmZm3Vf1TGEL4C5JtwDP9YyMiPfXEpWZmXVF1aRwSp1BmJnZuqHqLak3SdoGmBQRv8jfZh5ab2hmZjbQqj46+0jgCuCcPGoscHVNMZmZWZdUvdD8aeAdwHIofnDHj7k2M1vPVE0Kz0XEyp6B/FgK355qZraeqZoUbpJ0ErBh/m3my4Fr6gvLzMy6oWpSmAEsBf4IfJL06Iqmv7hmZmaDV9W7j14g/Rzn9+oNx8zMuqnqs48W0uQaQkS8pt8jMjOzrunNs496bAB8BNis/8MxM7NuqnRNISIeK/09GBFnAO+qNzQzMxtoVbuPdi4NDiGdOYyqJSIzM+uaqt1H3yy9XgXcB3y036MxM7Ouqnr30V51B2JmZt1Xtfvoc+2mR8S3+iccMzPrpt7cfbQLMDsPvw+4GVhUR1BmZtYdvfmRnZ0jYgWApFOAyyPiiLoCMzOzgVf1MRcTgJWl4ZXAxH6PxszMuqrqmcLFwC2Sfkz6ZvMHgYtqi8rMzLqi6t1Hp0m6Dvi7POrwiPh9fWGZmVk3VO0+AhgJLI+IbwOLJW1bU0xmZtYlVX+O8yvAicAX86jhwCV1BWVmZt1R9Uzhg8D7gacBImIJfsyFmdl6p2pSWBkRQX58tqSN6gvJzMy6pWpS+JGkc4BNJB0J/AL/4I6Z2Xqn491HkgT8EHgDsBx4PXByRPy85tjMzGyAdUwKERGSro6ItwBOBGZm67Gq3UdzJe1SayRmZtZ1Vb/RvBdwtKT7SHcgiXQSMaWuwMzMbOC1TQqSJkTEA8C0vsxc0j7At4GhwLkR8bUW5XYB5gL7R8QVfVmWmZmtvU5nCleTno56v6QrI+LDVWcsaShwFrA3sBi4VdLsiLirSbmvA9f3KnIzM+t3na4pqPT6Nb2c967Agoi4NyJWApcB05uU+wxwJfBoL+dvZmb9rFNSiBavqxjLmj/CsziPK0gaS/q29Mx2M5J0lKR5kuYtXbq0l2GYmVlVnZLCjpKWS1oBTMmvl0taIWl5h/eqybjGxHIGcGJErG43o4iYFRFTI2Lq6NGjOyzWzMz6qu01hYgYuhbzXgyMLw2PA5Y0lJkKXJa+H8cWwL6SVkXE1WuxXDMz66Oqt6T2xa3ApPyI7QeBA4CPlQtERPH4bUkXANc6IZiZdU9tSSEiVkk6hnRX0VDg/IiYL+noPL3tdQQzMxt4dZ4pEBFzgDkN45omg4g4rM5YzMyss9788pqZma3nnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs0KtSUHSPpLukbRA0owm0w+SdEf++42kHeuMx8zM2qstKUgaCpwFTAMmAwdKmtxQbCGwR0RMAU4FZtUVj5mZdVbnmcKuwIKIuDciVgKXAdPLBSLiNxHxeB6cC4yrMR4zM+ugzqQwFlhUGl6cx7XyCeC6ZhMkHSVpnqR5S5cu7ccQzcysrM6koCbjomlBaS9SUjix2fSImBURUyNi6ujRo/sxRDMzKxtW47wXA+NLw+OAJY2FJE0BzgWmRcRjNcZjZmYd1HmmcCswSdK2kkYABwCzywUkTQCuAg6JiD/XGIuZmVVQ25lCRKySdAxwPTAUOD8i5ks6Ok+fCZwMbA6cLQlgVURMrSsmMzNrr87uIyJiDjCnYdzM0usjgCPqjMHMzKrzN5rNzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMysUGtSkLSPpHskLZA0o8l0SfpOnn6HpJ3rjMfMzNqrLSlIGgqcBUwDJgMHSprcUGwaMCn/HQV8t654zMysszrPFHYFFkTEvRGxErgMmN5QZjpwUSRzgU0kbV1jTGZm1sawGuc9FlhUGl4M7FahzFjgoXIhSUeRziQAnpJ0T/+GWqstgGXdDmKAuc4d6Os1RjJwvJ0Hl22qFKozKajJuOhDGSJiFjCrP4IaaJLmRcTUbscxkFznlwfXef1UZ/fRYmB8aXgcsKQPZczMbIDUmRRuBSZJ2lbSCOAAYHZDmdnAx/NdSG8FnoyIhxpnZGZmA6O27qOIWCXpGOB6YChwfkTMl3R0nj4TmAPsCywAngEOryueLhqU3V5ryXV+eXCd10OKeEkXvpmZvUz5G81mZlZwUjAzs4KTQj+TtJmkn0v6S/6/aZuyQyX9XtK1Axljf6tSZ0njJf1a0t2S5kv6bDdiXVsvx0e3VKjzQbmud0j6jaQduxFnf+lU31K5XSStlvQ/BjK+ujkp9L8ZwC8jYhLwyzzcymeBuwckqnpVqfMq4PiI2B54K/DpJo89Wae9HB/dUrHOC4E9ImIKcCqD+GJsxfr2lPs66Uaa9YqTQv+bDlyYX18IfKBZIUnjgPcC5w5MWLXqWOeIeCgifpdfryAlw7EDFWA/eTk+uqVjnSPiNxHxeB6cS/q+0WBVZRsDfAa4Enh0IIMbCE4K/W/Lnu9a5P+vblHuDOAE4IUBiqtOVesMgKSJwJuB/6w/tH7V6rEsvS0zmPS2Pp8Arqs1onp1rK+kscAHgZkDGNeAqfMxF+stSb8Atmoy6UsV378f8GhE3CZpz34MrTZrW+fSfDYmfcI6LiKW90dsA6jfHt0yiFSuj6S9SElh91ojqleV+p4BnBgRq6VmxQc3J4U+iIj3tJom6RFJW0fEQ7nboNnp5TuA90vaF9gAeKWkSyLi4JpCXmv9UGckDSclhO9HxFU1hVqnl+OjWyrVR9IUUlfotIh4bIBiq0OV+k4FLssJYQtgX0mrIuLqAYmwZu4+6n+zgUPz60OBnzQWiIgvRsS4iJhIevzHr9blhFBBxzor7UHnAXdHxLcGMLb+9HJ8dEvHOkuaAFwFHBIRf+5CjP2pY30jYtuImJj33yuAT60vCQGcFOrwNWBvSX8B9s7DSBojaU5XI6tPlTq/AzgEeJek2/Pfvt0Jt28iYhXQ8+iWu4Ef9Ty6pefxLaRHt9xLenTL94BPdSXYflKxzicDmwNn5+06r0vhrrWK9V2v+TEXZmZW8JmCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBLJP0VM3zP07SyIFanllfOCmYDZzjgJGdCpl1kx9zYdaGpNeSHqU8mvQ74kdGxJ8kXQAsJz3yYCvghIi4QtIQ4ExgD9IjpYcA5wNj8t+vJS2LiL3y/E8D9gOeBaZHxCMDWT+zRj5TMGtvFvCZiHgL8Hng7NK0rUkPf9uP/C1u4EPARGAH4AjgbQAR8R3SM3T26kkIwEbA3IjYEbgZOLLWmphV4DMFsxbyE13fDlxeehrmK0pFro6IF4C7JG2Zx+0OXJ7HPyzp120WsRLo+dW920iPCDHrKicFs9aGAE9ExE4tpj9Xeq2G/1U8Hy8+Z2Y13h9tHeDuI7MW8u89LJT0ESh+f7nT7w//B/BhSUPy2cOepWkrgFG1BGvWT5wUzF40UtLi0t/ngIOAT0j6AzCf5j/NWHYl6Zn8dwLnkH5d7sk8bRZwXYcuJbOu8lNSzfqZpI0j4ilJmwO3AO+IiIe7HZdZFe7DNOt/10raBBgBnOqEYIOJzxTMzKzgawpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmaF/wYiSuq/Lto6UAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The error in the estimation of the std will affect the length of the asymptotic confidence interval. A larger error will result in a wider confidence interval.\n",
    "\n",
    "asymptotic_ci_lengths = 1.96 * bootstrap_std / np.sqrt(N)\n",
    "plt.hist(asymptotic_ci_lengths, bins=30)\n",
    "plt.title('Bootstrapped Lengths of Asymptotic Confidence Interval')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# the answers to the previous points depend on N. As N increases, the standard deviation estimated from the bootstrap method becomes more accurate, and the asymptotic confidence interval becomes narrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvgUlEQVR4nO3de5xVVf3/8dfbkauAysUboGhfvCDicBXDFNNSzMTr1wsJFoWkqWkXofoq9pUy0/RrZqamYt4gK7V+Vl7xihIaIogIKgqKiiQqCnL7/P7Ya8bDcGbmcJiZwzDv5+Mxjzln7XXWXmufdfbnrL33WVsRgZmZWTG2KHUFzMys8XIQMTOzojmImJlZ0RxEzMysaA4iZmZWNAcRMzMrmoNIiUkaJun+UtejgqRWkv4q6QNJfyx1fTaEpL9LGlFHZX1B0pyc5/MlHVoXZafyZkkaXFfl1QdJp0l6otT1aEokTZb0zfR4k9o3VGezCSKSTpE0TdIySYvSDuWAUterNhFxW0R8udT1yHE8sD3QISJOyF0g6dq0fZdJWilpVc7zv9dnpSSFpI/TupZIekjSibl5ImJIREwosKz/qilPRDweEXtsbL3T+m6WdHGV8veOiMl1UX4pSOqWtmPF+/+OpGskNauDsgsOXpIGS1q4sevcEJLG5fT9pZKekrR/Xa+nrvYNhfT3jbFZBBFJ5wFXAj8j2wHuDFwDDC1htWolactS1yGPXYCXI2J11QURMToi2kREG7JtPbHieUQMqchXj+3aN617D+Bm4GpJF9b1SjbR92VTtU16T/YB9gfOLHF91lNP7+fE1O6OwCNAoxq116mIaNR/wNbAMuCEGvK0IAsyb6W/K4EWadlgYCHwQ+BdYBFwNHAE8DLwH+BHOWWNA+4CJgIfAc+R7dwqlo8BXknLXgSOyVl2GvAkcEUq9+KU9kRarrTsXeADYAbQM6edtwCLgdeBnwBb5JT7BHAZ8D7wGjCkhu2xFzAZWArMAo5K6RcBK4FVaZuOrKGMccCtOc/nA+enOn8KbAkE8F85eW4GLs55fiQwPdXjKaBXDetbp6yUdjywgmzURGrTN9Pj/wIeTdvxPbIPPcBjqayPUxtPzOkD5wNvA3+oSKvSvrHpPX0fuAlombv989UXGJW258q0vr/mlHfoBvTP7/FZ//x6Ddvp68Bssv73KnB6zrIaywI6APcCHwJTgf+t2q6cvN1SG7fMSbsUuK62flZTf06vWQGsSdtracp/RNr2HwFvAt8HtgKWA2tT3mXATnz2Gb01teWbwABgSqrLIuBqoHmV9+vstM3eA35J+nwV0Pd7pNd3ymnb79N63iT7nJdV2Qf8mqxvvgQcklPWZD7rw6flbn9gb+ABsn3HO6T9Uk1tI09/r+2zR/Y5eDNt6zm59cu7Pepjx96Qf8DhwOrczpwnz0+Bp4HtgE5po/1vzgdrNXAB0Az4VurYtwNt0xu3AtgtpwOtItuBNUud+TWgWVp+QurIW5DtoD4GdszpFKuBs8h2sq1YN4gcBjwLbEMWUPbKee0twD2pTt3IAtzInHJXpbqXAd8m2xkpz7ZoBswDfgQ0B76YOsse+T4gNWzTdfKR7RSnA12BVjkfzLxBBOhDtiPbL9V5RCqjRTXryxdEmqXtOSTPB/AO4MfpfWgJHFBdWTl94BdkO/RW5A8iM1P72pPtCCraUvke5lsHVYJnTnkVQaSQ/vnT1N4jgE+AbavZTl8BPkfWfw5KefsUUhZwJzCJbOfck2xHUlAQIevzzwPfKLCf1dafq27PRcAX0uNtq7RpYZW848g+D0en978V0BcYSPa560YWaL9b5f16JL23O6f6fLO2vp/adglZ4KnYFncDv0vbcTuygHx6lX3AuWkbnUgWTNrn6cOV2yFtp0VkXwBapuf7pWWFtC23v1f72SMb5S8Adsp5nz9X476grnfqDf0HDAPeriXPK8AROc8PA+bndMLlfPZNoW3a6Pvl5H8WODqnAz2ds2yL3A6eZ93TgaE5neKNKstzO8oXU+cdSM63oPRGfwr0yEk7HZicU8a8nGWtUxt2yFOfL5B9284t/w5gXNUPSC3bdJ18qRN+o0qemoLIb0k7ypzlc4CDqlnfekEkpb8NDEuPJ/PZB/AW4DqgS21lpT6wkjSyyEmrGkRG5zw/Anil6nuYbx3UHkQK6Z+53/jfBQYW+Pm4GzintrJSH1sF7Jmz7GdV25WzrFtq49L0F2TBr11t/YzC+nPV7flGytOuSvo671NO33yslu3yXeAvVd6vw3OenwE8VEPfX5navQZYAgxOy7ZPbWuVk/9k4JGctq3zBY8syJyapw9XbodUxr8LfM/ztS23v1f72SMbPb8LHEr6Ylzb3+ZwTmQJ0LGW4547kQ2ZK7ye0irLiIg16fHy9P+dnOXLgTY5zxdUPIiItWSHCHYCkDRc0vR0wm0p2Te6jvleW1VEPEw2FP0N8I6k6yS1S69vnqcNnXOev51TzifpYW6dK+wELEj1rq6sYlXbtjx2Ab5XsZ3SturKuu9LjdJJ3E5kw/uqfkj2bXxquhLqG7UUtzgiVtSSJ7d9VfvQxiikf+aeo/qE/O8tkoZIelrSf9I2PYJ1+191ZXUi+yZbtY216RgR25B9cXkS+EdOm6rrZ4X056qOS215XdKjBZzIXqcvStpd0t8kvS3pQ7IA2bGG19T2/k5K7d6ebITaN6XvQjbCWJTTr39HNiKp8GakPXeB64Lss/FKvgUFti1XtZ+9iJhHFoTGAe9KulNSjXXbHILIFLLDTUfXkOctsg1XYeeUVqyuFQ8kbQF0Ad6StAtwPfAdsuP025B1MOW8NrfzrCciroqIvmSH0XYHfkA2VF6Vpw1vFlH3t4Cuqd4bW1ZVVdv2CdnOpcIOOY8XAOMjYpucv9YRcccGrG8o2aGBqetVJOLtiPhWROxE9g32mlquUKnxfUm65jzO7UMfk9NOSbntLKTsOumfkloAfyI7N7Z96n/3sW7/q85ism1ZtY0FiYjlZCOu/SV1pOZ+Vlt/Xm97RcS/ImIo2c74brLDbnnzVpP+W7LzD90joh3ZYbaq26W697daEfEeWf8aJ2lHsn79KSm4pr92EbF3zss6S8pddyHrWkB2mDKfQtpWtaxqP3sRcXtEHED2/gTZYd5qNfogEhEfkJ3P+I2koyW1ltQsfSO7NGW7A/iJpE6pg19AdtKtWH0lHZtGP98l6zRPkx0DDbIPJJK+TjYSKYik/pL2S9+wPyadYEyjpEnAeEltU7A6r8g2PJPK/mHaToOBr5IdD69r04FTJJVJOpxsuFzhemB0aq8kbSXpK5La1laopPaShpGN2H4REUvy5DlBUpf09H2y96VitPkOsFsR7TlTUhdJ7ck+qBNT+vPA3pLKJbUk+xaXq7b11VX/bE52XHsxsFrSEKCgS0RTH/sz2c6wtaQeZMfKC5IC2KlkI+Il1NDPCujP7wBdJDVPZTdX9puJrSNiFdnJ8tz3soOkrWupYtv0umWS9iQ7b1jVDyRtK6krcA6fvb81ioiXgH8CP4yIRcD9wOWS2knaQtLnJOX2/e2As9N2OYHs3Od9tazmb8AOkr4rqUXabvsV2Laq/a/az56kPSR9Mb2fK8iOwqyhBo0+iABExK/IOuFPyD5AC8hGA3enLBcD08iuHHqB7Iqqi9crqHD3kJ0Qe5/sg3NsRKyKiBeBy8lGR++QXfb45AaU247sDX6fbIi7hOxbJWQn4z8mu3rkCbIT/zduaMUjYiVwFDCE7BvhNcDw9EGoa+eQ7TiWkp27ujunHtPILgS4mqy988iOAdfkeUnLUt5vAudGxAXV5O0PPJPy30t2XuC1tGwcMCEN5f97A9pzO9kO4tX0d3Fqy8tkJ6sfBOaSvT+5fg/0SOu7O0+5ddI/I+IjsiuMJpFt01PI2l6o75Ad2nqbbFRxUwGvWZq28Ttkl/geFZna+llN/flhsqu53pb0Xko7FZifDteMBr6W2vwSWRB+NW3f6g69fJ9se3xE9hnLFyDuITv/OR34f2TvW6F+CYyStB0wnCygV1zJdxewY07eZ4DuZNtlPHB8vi9CudJ7+yWyz9PbZP3s4ALbNo6c/l7LZ68Fn10o8DZZwPtRTXXTuofmrDaSxpGdpPpaqetiZnVDUpAdDppXz+s5jezE+Sb/Q+hCbRYjETMzKw0HETMzK5oPZ5mZWdE8EjEzs6JtthPNdezYMbp161bqapiZNSrPPvvsexHRqdD8m20Q6datG9OmTSt1NczMGhVJhcxUUMmHs8zMrGgOImZmVjQHETMzK9pme04kn1WrVrFw4UJWrKhtslZrylq2bEmXLl1o1myj7/RqttlrUkFk4cKFtG3blm7durHuJJpmmYhgyZIlLFy4kF133bXU1THb5DWpw1krVqygQ4cODiBWLUl06NDBo1WzAjWpIAI4gFit3EfMCtfkgoiZmdWdJnVOpKorHni5Tss790u717h8yZIlHHLIIQC8/fbblJWV0alT9sPQqVOn0rx5842uw+DBg1m0aBEtWrRg5cqVHHrooVx88cVss802AHz+85/nqaeeqvb1P/vZz/jRj6q/fcARRxzB7bffztKlSznyyCOZOXNmwXWbPHkyzZs35/Of/zwA1157La1bt2b48OEFl2FmmxaPRBpQhw4dmD59OtOnT2f06NGce+65lc+bN2/O6tWray+kALfddhszZsxgxowZtGjRgqFDh1YuqymAQBZE8okI1q5dy3333VcZkDbU5MmT11n/6NGjHUAaWF1/cTJzECmx0047jfPOO4+DDz6Y888/n3HjxnHZZZdVLu/Zsyfz588H4NZbb2XAgAGUl5dz+umns2ZNjXetpHnz5lx66aW88cYbPP/88wC0adMGgEWLFnHggQdSXl5Oz549efzxxxkzZgzLly+nvLycYcOGMX/+fPbaay/OOOMM+vTpw4IFC+jWrRvvvZfdbG716tWMGDGCXr16cfzxx/PJJ58ArJNn2rRpDB48mPnz53PttddyxRVXUF5ezuOPP75OW6dPn87AgQPp1asXxxxzDO+//z6QjazOP/98BgwYwO67787jjz9eR1vezOqCg8gm4OWXX+bBBx/k8ssvrzbP7NmzmThxIk8++STTp0+nrKyM2267rdayy8rK2HfffXnppXXvfnv77bdz2GGHMX36dJ5//nnKy8u55JJLaNWqFdOnT68se86cOQwfPpx///vf7LLLLuuUMWfOHEaNGsWMGTNo164d11xzTbX16Nat2zqjry984QvrLB8+fDi/+MUvmDFjBvvssw8XXXRR5bLVq1czdepUrrzyynXSzaz0HEQ2ASeccAJlZWU15nnooYd49tln6d+/P+Xl5Tz00EO8+uqrBZWf754x/fv356abbmLcuHG88MILtG3bNu9rd9llFwYOHJh3WdeuXRk0aBAAX/va13jiiaq3Fi/MBx98wNKlSznooIMAGDFiBI899ljl8mOPPRaAvn37Vo7KzGzT4CCyCdhqq60qH2+55ZasXbu28nnF7xUighEjRlSeQ5kzZw7jxo2rtew1a9bwwgsvsNdee62TfuCBB/LYY4/RuXNnTj31VG655ZZa61ZV1UthK57ntqEufm/RokULIBtV1dV5IzOrGw4im5hu3brx3HPPAfDcc8/x2muvAXDIIYdw11138e677wLwn//8h9dfr3nG5lWrVjF27Fi6du1Kr1691ln2+uuvs9122/Gtb32LkSNHVq6zWbNmrFq1qqC6vvHGG0yZMgWAO+64gwMOOKCyDc8++ywAf/rTnyrzt23blo8++mi9crbeemu23XbbyvMdf/jDHypHJWa2aau3S3wl3QgcCbwbET1T2i+BrwIrgVeAr0fE0rRsLDASWAOcHRH/TOl9gZuBVsB9wDlRR/f0re2S3FI47rjjuOWWWygvL6d///7svntWxx49enDxxRfz5S9/mbVr19KsWTN+85vfrHeeAmDYsGG0aNGCTz/9lEMPPZR77rlnvTyTJ0/ml7/8Jc2aNaNNmzaVI5FRo0bRq1cv+vTpw/jx42us61577cWECRM4/fTT6d69O9/+9rcBuPDCCxk5ciQ/+9nP2G+//Srzf/WrX+X444/nnnvu4de//vU6ZU2YMIHRo0fzySefsNtuu3HTTTdt2IYzs5Kot3usSzoQWAbckhNEvgw8HBGrJf0CICLOl9QDuAMYAOwEPAjsHhFrJE0FzgGeJgsiV0XE32tbf79+/aLqTalmz5693mEds3w2175yxQMvb5JfnmzTIenZiOhXaP56O5wVEY8B/6mSdn9EVBzUfhrokh4PBe6MiE8j4jVgHjBA0o5Au4iYkkYftwBH11edzcxsw5TynMg3gIoRRWdgQc6yhSmtc3pcNd3MzDYBJQkikn4MrAYqfuiQb8a7qCG9unJHSZomadrixYs3vqJmZlajBg8ikkaQnXAflnOCfCHQNSdbF+CtlN4lT3peEXFdRPSLiH4Vc1KZmVn9adAgIulw4HzgqIj4JGfRvcBJklpI2hXoDkyNiEXAR5IGKvsRwnBg/UuNzMysJOrzEt87gMFAR0kLgQuBsUAL4IH0w7SnI2J0RMySNAl4keww15kRUTEx1Lf57BLfv/PZeRQzK5AnXrT6Um9BJCJOzpP8+xryjwfW+2FCREwDetZh1T7zyM/rtryDx9aapaysjH322YeIoKysjKuvvrpyavQNMX/+fJ566ilOOeWUavMsXbqU22+/nTPOOGODyy/EuHHjuP766+nUqRMrV67kf/7nfzj55Hxve+0GDx7MZZddRr9+/Sqnm9/Q2YKrTjVvZvXPv1hvYBUTHD7//PP8/Oc/Z+zY2gNPPvPnz+f222+vMc/SpUurnRSxthmAC1UxoeI999zD6aefXvCv3WtS7HTzVaeaN7P65yBSQh9++CHbbrstkM2N9YMf/ICePXuyzz77MHHixBrTx4wZw+OPP055eTlXXHEFs2bNqpwmvlevXsydO5cxY8bwyiuvUF5ezg9+8AMmT57MwQcfzCmnnMI+++wDwNFHH03fvn3Ze++9ue666yrr1qZNG773ve/Rp08fDjnkEGq72q179+60bt26cgr3X/7yl/Tv359evXpx4YUXAlng23PPPfNOH58rdyr5W265hV69erHvvvty6qmnAvDXv/6V/fbbj969e3PooYfyzjvv5J1qfvHixRx33HH079+f/v378+STTwLw6KOPUl5eTnl5Ob179847FYuZFaZJ39mwFCru17FixQoWLVrEww8/DMCf//znyhHKe++9R//+/TnwwAN56qmn8qZfcsklXHbZZfztb38D4KyzzuKcc85h2LBhrFy5kjVr1nDJJZcwc+ZMpk+fDmTf1KdOncrMmTPZddddAbjxxhtp3749y5cvp3///hx33HF06NCBjz/+mD59+nD55Zfz05/+lIsuuoirr7662nY999xzdO/ene22247777+fuXPnMnXqVCKCo446iscee4ydd96ZOXPm8Pvf/55BgwbxjW98g2uuuYbvf//7ecucNWsW48eP58knn6Rjx4785z/Zb1cPOOAAnn76aSRxww03cOmll3L55ZczevRo2rRpU1neKaecwrnnnssBBxzAG2+8wWGHHcbs2bO57LLL+M1vfsOgQYNYtmwZLVu2rJP31qwpchBpYBWHswCmTJnC8OHDmTlzJk888QQnn3wyZWVlbL/99hx00EH861//qja9Xbt265S7//77M378eBYuXMixxx5L9+7d865/wIABlQEE4KqrruIvf/kLAAsWLGDu3Ll06NCBLbbYghNPPBHIpnmvmI69qiuuuILrr7+eV199lX/84x8A3H///dx///307t0bgGXLljF37lx23nnn9aaPv+qqq6oNIg8//DDHH388HTt2BKB9+/YALFy4kBNPPJFFixaxcuXKddqT68EHH+TFF1+sfP7hhx/y0UcfMWjQIM477zyGDRvGscceS5cuXfK+fnPlqU+sLvlwVgntv//+vPfeeyxevDjvPT8g/71A8jnllFO49957adWqFYcddljlCKeq3KndJ0+ezIMPPsiUKVN4/vnn6d27d7VTt1ed9r3Cueeey5w5c5g4cSLDhw9nxYoVRARjx46tnLZ+3rx5jBw5Mm851ZULWdvzLT/rrLP4zne+wwsvvMDvfve7auu8du1apkyZUlmPN998k7Zt2zJmzBhuuOEGli9fzsCBA9e7YZeZFc5BpIReeukl1qxZQ4cOHTjwwAOZOHEia9asYfHixTz22GMMGDCg2vSq06q/+uqr7Lbbbpx99tkcddRRzJgxo9qp1yt88MEHbLvttrRu3ZqXXnqJp59+unLZ2rVrueuuu4DsLogV07xX59hjj6Vfv35MmDCBww47jBtvvJFly5YB8Oabb1ZOYV/d9PH5HHLIIUyaNIklS5YAVB7O+uCDD+jcOZv9ZsKECZX5q7b3y1/+8jqH4CpGgK+88gr77LMP559/Pv369XMQMdsITftwVgGX5Na1inMikH3TnjBhAmVlZRxzzDFMmTKFfffdF0lceuml7LDDDtWmd+jQgS233JJ9992X0047jRUrVnDrrbfSrFkzdthhBy644ALat2/PoEGD6NmzJ0OGDOErX/nKOnU5/PDDufbaa+nVqxd77LHHOncw3GqrrZg1axZ9+/Zl6623rjyhX5MLLriAU045hdmzZzN79mz2339/IDtJf+utt1JWVlbt9PH57L333vz4xz/moIMOoqysjN69e3PzzTczbtw4TjjhBDp37szAgQMr77lSdar5q666ijPPPJNevXqxevVqDjzwQK699lquvPJKHnnkEcrKyujRowdDhgzZ0LfRzJJ6mwq+1DwV/MZp06ZN5UiirsyfP58jjzySmTNn1mm59WFz6ytVf2zocyJWnU1mKngzM9v8OYhYXnU9CoHs9x+NYRRiZoVrckFkcz18Z3XHfcSscE0qiLRs2ZIlS5Z4J2HVigiWLFniHyCaFahJXZ3VpUsXFi5cWOsUHta0tWzZssn9ANGsWE0qiDRr1qzaXzebmdmGa1KHs8zMrG45iJiZWdEcRMzMrGgOImZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYmZmRXMQMTOzojmImJlZ0eotiEi6UdK7kmbmpLWX9ICkuen/tjnLxkqaJ2mOpMNy0vtKeiEtu0o13ZTbzMwaVH2ORG4GDq+SNgZ4KCK6Aw+l50jqAZwE7J1ec42ksvSa3wKjgO7pr2qZZmZWIvUWRCLiMeA/VZKHAhPS4wnA0Tnpd0bEpxHxGjAPGCBpR6BdREyJbP72W3JeY2YFqHprXLO61NDnRLaPiEUA6f92Kb0zsCAn38KU1jk9rpqel6RRkqZJmubp3s3M6t+mcmI933mOqCE9r4i4LiL6RUS/Tp061VnlzMwsv4YOIu+kQ1Sk/++m9IVA15x8XYC3UnqXPOlmZrYJaOggci8wIj0eAdyTk36SpBaSdiU7gT41HfL6SNLAdFXW8JzXmJlZidXbnQ0l3QEMBjpKWghcCFwCTJI0EngDOAEgImZJmgS8CKwGzoyINamob5Nd6dUK+Hv6MzOzTUC9BZGIOLmaRYdUk388MD5P+jSgZx1WzczM6simcmLdzMwaIQcRMzMrmoOIWRN0xQMv+0eIViccRMzMrGgOImZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYmZmRXMQMTOzojmImJlZ0RxEzMysaA4iZmZWNAcRMzMrmoOImZkVzUHEzMyK5iBiZmZFcxAxM7OiOYiYmVnRHETMzKxoDiJmZlY0BxEzMyuag4iZmRXNQcTMzIpWkiAi6VxJsyTNlHSHpJaS2kt6QNLc9H/bnPxjJc2TNEfSYaWos5mZra/Bg4ikzsDZQL+I6AmUAScBY4CHIqI78FB6jqQeafnewOHANZLKGrreZpujKx54udRVsEauVIeztgRaSdoSaA28BQwFJqTlE4Cj0+OhwJ0R8WlEvAbMAwY0bHXNzCyfgoKIpJ51tcKIeBO4DHgDWAR8EBH3A9tHxKKUZxGwXXpJZ2BBThELU1q+eo6SNE3StMWLF9dVlc0aLY80rL4VOhK5VtJUSWdI2mZjVpjOdQwFdgV2AraS9LWaXpInLfJljIjrIqJfRPTr1KnTxlTTzMwKUFAQiYgDgGFAV2CapNslfanIdR4KvBYRiyNiFfBn4PPAO5J2BEj/3035F6b1VuhCdvjLzMxKrOBzIhExF/gJcD5wEHCVpJckHbuB63wDGCiptSQBhwCzgXuBESnPCOCe9Phe4CRJLSTtCnQHpm7gOs3MrB5sWUgmSb2ArwNfAR4AvhoRz0naCZhCNpooSEQ8I+ku4DlgNfBv4DqgDTBJ0kiyQHNCyj9L0iTgxZT/zIhYU+j6zMys/hQURICrgeuBH0XE8orEiHhL0k82dKURcSFwYZXkT8lGJfnyjwfGb+h6zMysfhUaRI4AlleMACRtAbSMiE8i4g/1VjszM9ukFXpO5EGgVc7z1inNzMyasEKDSMuIWFbxJD1uXT9VMjOzxqLQIPKxpD4VTyT1BZbXkN/MzJqAQs+JfBf4o6SK32fsCJxYLzUyM7NGo6AgEhH/krQnsAfZL8hfSj8UNDOzJqzQkQhAf6Bbek1vSUTELfVSKzMzaxQK/bHhH4DPAdOBih/6BeAgYmbWhBU6EukH9IiIvBMfmplZ01To1VkzgR3qsyJmZtb4FDoS6Qi8KGkq2fQkAETEUfVSKzMzaxQKDSLj6rMSZmbWOBV6ie+jknYBukfEg5Jak90b3czMmrBCb4/7LeAu4HcpqTNwdz3VyczMGolCT6yfCQwCPoTKG1RtV+MrzMxss1doEPk0IlZWPJG0JdXc59zMzJqOQoPIo5J+BLRK91b/I/DX+quWmZk1BoUGkTHAYuAF4HTgPrL7rZuZWRNW6NVZa8luj3t9/VbHzMwak0LnznqNPOdAImK3Oq+RmZk1Ghsyd1aFlsAJQPu6r46ZmTUmBZ0TiYglOX9vRsSVwBfrt2pmZrapK/RwVp+cp1uQjUza1kuNzMys0Sj0cNblOY9XA/OB/67z2piZWaNS6NVZB9flSiVtA9wA9CQ7Yf8NYA4wkezuifOB/46I91P+scBIshtinR0R/6zL+piZWXEKPZx1Xk3LI+JXG7je/wP+ERHHS2oOtAZ+BDwUEZdIGkP225TzJfUATgL2BnYCHpS0e0Ssqa5wMyvcFQ+8zLlf2r3U1bBGqtAfG/YDvk028WJnYDTQg+y8yAadG5HUDjgQ+D1ARKyMiKXAUGBCyjYBODo9HgrcGRGfRsRrwDxgwIas08zM6seG3JSqT0R8BCBpHPDHiPhmEevcjezX7zdJ2hd4FjgH2D4iFgFExCJJFRM8dgaeznn9wpS2HkmjgFEAO++8cxFVMzOzDVHoSGRnYGXO85Vk5y6KsSXQB/htRPQGPiY7dFUd5UnLO/ljRFwXEf0iol+nTp2KrJ6ZmRWq0JHIH4Cpkv5CtgM/BrilyHUuBBZGxDPp+V1kQeQdSTumUciOwLs5+bvmvL4L8FaR6zYzszpU6I8NxwNfB94HlgJfj4ifFbPCiHgbWCBpj5R0CPAicC8wIqWNAO5Jj+8FTpLUQtKuQHdgajHrNmtKrnjg5VJXwZqAQkcikF1B9WFE3CSpk6Rd04nuYpwF3JauzHqVLEBtAUySNBJ4g2xqFSJilqRJZIFmNXCmr8wyM9s0FHqJ74VkV2jtAdwENANuJbvb4QaLiOmsOx9XhUOqyT8eGF/MuszMrP4UemL9GOAospPgRMRbeNoTM7Mmr9AgsjIignRVlKSt6q9KZmbWWBQaRCZJ+h2wjaRvAQ/iG1SZmTV5tZ4TkSSyOa32BD4kOy9yQUQ8UM91MzOzTVytQSQiQtLdEdEXcOAwM7NKhR7OelpS/3qtiZmZNTqF/k7kYGC0pPlkV2iJbJDSq74qZmZmm74ag4iknSPiDWBIA9XHzMwakdpGIneTzd77uqQ/RcRxDVAnMzNrJGo7J5I7g+5u9VkRMzNrfGoLIlHNYzMzs1oPZ+0r6UOyEUmr9Bg+O7Herl5rZ2Zmm7Qag0hElDVURczMrPEp9HciZmZm63EQMTOzojmImJlZ0RxEzMysaA4iZmZWNAcRMzMrmoOImZkVzUHEzMyK5iBiZmZFcxAxM7OiOYiYmVnRShZEJJVJ+rekv6Xn7SU9IGlu+r9tTt6xkuZJmiPpsFLV2czM1lXKkcg5wOyc52OAhyKiO/BQeo6kHsBJwN7A4cA1kjwxpJnZJqAkQURSF+ArwA05yUOBCenxBODonPQ7I+LTiHgNmAcMaKCqmplZDUo1ErkS+CGwNidt+4hYBJD+b5fSOwMLcvItTGnrkTRK0jRJ0xYvXlznlTYzs3U1eBCRdCTwbkQ8W+hL8qTlvctiRFwXEf0iol+nTp2KrqOZmRWmtjsb1odBwFGSjgBaAu0k3Qq8I2nHiFgkaUfg3ZR/IdA15/VdgLcatMZmZpZXg49EImJsRHSJiG5kJ8wfjoivAfcCI1K2EcA96fG9wEmSWkjaFegOTG3gapuZWR6lGIlU5xJgkqSRwBvACQARMUvSJOBFYDVwZkSsKV01zcysgiLynl5o9Pr16xfTpk0rdTXMSuaKB17eoPznfmn3eqqJNSaSno2IfoXm9y/WzcysaJvS4SwzqwMbOgIx2xgeiZiZWdEcRMzMrGgOImZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYmZmRXMQMTOzojmImJlZ0RxEzMysaA4iZmZWNAcRMzMrmoOImZkVzUHEzMyK5iBiZmZFcxAxM7OiOYiYmVnRHETMzKxoDiJmBmS31fWtdW1DOYiYmVnRHETMzKxoDiJmZla0Bg8ikrpKekTSbEmzJJ2T0ttLekDS3PR/25zXjJU0T9IcSYc1dJ3NzCy/UoxEVgPfi4i9gIHAmZJ6AGOAhyKiO/BQek5adhKwN3A4cI2kshLU28zMqmjwIBIRiyLiufT4I2A20BkYCkxI2SYAR6fHQ4E7I+LTiHgNmAcMaNBKm5lZXiU9JyKpG9AbeAbYPiIWQRZogO1Sts7AgpyXLUxp+cobJWmapGmLFy+ut3qbbap8ia41tJIFEUltgD8B342ID2vKmict8mWMiOsiol9E9OvUqVNdVNPMzGpQkiAiqRlZALktIv6ckt+RtGNaviPwbkpfCHTNeXkX4K2GqquZmVWvFFdnCfg9MDsifpWz6F5gRHo8ArgnJ/0kSS0k7Qp0B6Y2VH3NzKx6W5ZgnYOAU4EXJE1PaT8CLgEmSRoJvAGcABARsyRNAl4ku7LrzIhY0+C1NjOz9TR4EImIJ8h/ngPgkGpeMx4YX2+VMjOzovgX62ZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYmZmRXMQMTOzojmImNk6PP+WbQgHETMzK5qDiJmZFc1BxMzMiuYgYraZ8LkMKwUHETMzK5qDiJmtx6MaK5SDiJmZFc1BxMzMiuYgYmZmRXMQMTOzojmImJlZ0RxEzMysaA4iZpaXL/O1QjiImG0GvMO3UnEQMTOzojmImJlZ0RxEzMysaFuWugKFknQ48H9AGXBDRFxS4iqZlVx9nwvJLf/cL+1er+uyxqlRjEQklQG/AYYAPYCTJfUoba3MzKxRBBFgADAvIl6NiJXAncDQEtfJrKQa+oosXwFm+TSWw1mdgQU5zxcC+1XNJGkUMCo9XSZpTgPUrVgdgfdKXYkG1tTavNm197zas2x2bS7A5tbmXTYkc2MJIsqTFuslRFwHXFf/1dl4kqZFRL9S16MhNbU2N7X2gtvcFDWWw1kLga45z7sAb5WoLmZmljSWIPIvoLukXSU1B04C7i1xnczMmrxGcTgrIlZL+g7wT7JLfG+MiFklrtbGahSH3epYU2tzU2svuM1NjiLWO7VgZmZWkMZyOMvMzDZBDiJmZlY0B5EGIqm9pAckzU3/t60hb5mkf0v6W0PWsS4V0l5JXSU9Imm2pFmSzilFXTeWpMMlzZE0T9KYPMsl6aq0fIakPqWoZ10qoM3DUltnSHpK0r6lqGddqq3NOfn6S1oj6fiGrF+pOIg0nDHAQxHRHXgoPa/OOcDsBqlV/SmkvauB70XEXsBA4MzGNp1NgVPyDAG6p79RwG8btJJ1rMA2vwYcFBG9gP+lkZ98LnTqpZTvF2QXATUJDiINZygwIT2eABydL5OkLsBXgBsaplr1ptb2RsSiiHguPf6ILHB2bqgK1pFCpuQZCtwSmaeBbSTt2NAVrUO1tjkinoqI99PTp8l+29WYFTr10lnAn4B3G7JypeQg0nC2j4hFkO08ge2qyXcl8ENgbQPVq74U2l4AJHUDegPP1H/V6lS+KXmqBsJC8jQmG9qekcDf67VG9a/WNkvqDBwDXNuA9Sq5RvE7kcZC0oPADnkW/bjA1x8JvBsRz0oaXIdVqxcb296cctqQfXv7bkR8WBd1a0CFTMlT0LQ9jUjB7ZF0MFkQOaBea1T/CmnzlcD5EbFGypd98+QgUoci4tDqlkl6R9KOEbEoHcrIN9wdBBwl6QigJdBO0q0R8bV6qvJGqYP2IqkZWQC5LSL+XE9VrU+FTMmzuU3bU1B7JPUiOyw7JCKWNFDd6kshbe4H3JkCSEfgCEmrI+LuBqlhifhwVsO5FxiRHo8A7qmaISLGRkSXiOhGNrXLw5tqAClAre1V9mn7PTA7In7VgHWrS4VMyXMvMDxdpTUQ+KDiUF8jVWubJe0M/Bk4NSI2hznka21zROwaEd3S5/cu4IzNPYCAg0hDugT4kqS5wJfScyTtJOm+ktasfhTS3kHAqcAXJU1Pf0eUprrFiYjVQMWUPLOBSRExS9JoSaNTtvuAV4F5wPXAGSWpbB0psM0XAB2Aa9L7Oq1E1a0TBba5SfK0J2ZmVjSPRMzMrGgOImZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYo2SpB+nmX9npEtI90vp35XUug7XM19Sx414/WmSrq4mfXGarXmupH9K+vxGrOenkqr98WfOOnfKeX5DY5vw0jY9/sW6NTqS9geOBPpExKdpJ988Lf4ucCvwSYnqVhYRawrMPjEivpNedzDwZ0kHR8QGz+AcERcUkO00YCbpl9YR8c0NXY9ZVR6JWGO0I/BeRHwKEBHvRcRbks4GdgIekfQIgKTfSpqWRi0XVRSQRhgXSXpO0guS9kzpHSTdn0YIvyNnziRJd0t6NpU1Kid9WRoJPAPsL+nrkl6W9CjZDyprFRGPkE2XPiqV+TlJ/0jre1zSnpK2TvXeIuVpLWmBpGaSbla6f4WkCyT9S9JMSdelX8ofTzYtx21p5NZK0mRJ/dJrTk7bYaakX1Rp23hJz0t6WtL2G/pm2ebNQcQao/uBrmlHfY2kgwAi4iqyb9kHR8TBKe+PI6If0As4SNl8ThXei4g+ZPf3+H5KuxB4IiJ6k01rsXNO/m9ERF+ynfHZkjqk9K2AmRGxH/AKcBFZ8PgS2b0nCvUcsGd6fB1wVlrf94FrIuID4HngoJTnq8A/I2JVlXKujoj+EdETaAUcGRF3AdOAYRFRHhHLKzKnQ1y/AL4IlAP9JR2d07anI2Jf4DHgWxvQHmsCHESs0YmIZUBfsm/ti4GJkk6rJvt/S3oO+DewN+vu1CsmfHwW6JYeH0h2OIyI+H/A+zn5z5b0PNn9MbqS3WQKYA3ZJJIA+wGTI2Jxuu/ExA1omqByVuPPA3+UNB34Hdnoi1TeienxSdWUf7CkZyS9QBYY9q5lvf1z6rwauI1sOwCsBCrusJm7ncwAnxOxRiqdd5gMTE47yxHAzbl5JO1K9i2+f0S8L+lmstmRK3ya/q9h3c/CenMBKZua/1Bg/4j4RNLknLJWVDkPUuxcQr3J5mXaAlgaEeV58twL/FxSe7JA+nCVerYErgH6RcQCSeNYt8351DRv+ar4bG6kqtvJzCMRa3wk7SGpe05SOfB6evwR0DY9bgd8DHyQjuUPKaD4x4BhaT1DgIp7w28NvJ8CyJ5kt/PN5xlgcDq30gw4ocA2HUQ2sro+3VPlNUknpGVSukd5GoVNBf4P+Fuek/gVAeO9NKLJvc937rapWueDJHVUdnvXk4FHC6m3mb9VWGPUBvi1pG3I7tM+j3RCmuxcwt8lLYqIgyX9G5hFNovukwWUfRFwRzoE9ijwRkr/BzBa0gxgDtkhrfWk+6eMA6YAi8jOc5RVs64TJR0AtCa7J/lxOVdmDQN+K+knQDOy27E+n5ZNBP4IDM6z/qWSrgdeAOaTTWFe4WbgWknLgf2r1Hks8AjZqOS+iFhv6n6zfDyLr5mZFc2Hs8zMrGgOImZmVjQHETMzK5qDiJmZFc1BxMzMiuYgYmZmRXMQMTOzov1/nIxsTlvRd24AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = 10000\n",
    "true_std = []\n",
    "for i in range(R):\n",
    "    sample = human_protein_lengths.sample(n=N)\n",
    "    true_std.append(np.std(np.log(sample['LogLength'])))\n",
    "\n",
    "plt.hist(true_std, bins=30, alpha=0.5, label='True Distribution')\n",
    "plt.hist(bootstrap_std, bins=30, alpha=0.5, label='Bootstrap Replicates')\n",
    "plt.title('Comparison of True Distribution and Bootstrap Replicates')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the R values to estimate the expected value and standard deviation of the estimator.\n",
    "true_mean = np.mean(true_std)\n",
    "true_std = np.std(true_std)\n",
    "print(f\"True Mean: {true_mean:.2f}, True Standard Deviation: {true_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm_Hq6hbl8yF"
   },
   "source": [
    "**Exercise 5.** In this exercise, we will use the bootstrap to estimate the bias of the estimator of standard deviation. It works like this. Let $X = X_1, \\dots, X_N$ be a random sample from a population, and let $\\hat\\sigma(X)$ be the estimator of the standard deviation $\\hat\\sigma(X) = \\sqrt{\\sum (X_i - \\bar X)^2 / N}$. Let $X^*_1, \\dots, X^*_B$ be the bootstrap replicates from $X$, where $X^*_1 = X^*_{1,1}, \\dots, X^*_{1, N}$. To estimate the bias of $\\hat\\sigma$, we simply calculate\n",
    "\n",
    "$$\\frac{1}{B}\\sum_{i=1}^B (\\hat\\sigma(X^*_i) - \\hat\\sigma(X)).$$\n",
    "\n",
    "*Warning.* This procedure works for a broad class of estimators (like the common estimators of the standard deviation or the variance), but not for all of them. You should be careful if you want to use it in your own analyses. You can read more in the book *An Introduction to the Bootstrap* by Efron and Tibshirani.  \n",
    "\n",
    "1. Estimate the bias of $\\hat\\sigma$ using the bootstrap replicates from the previous Exercise. Does the result suggest that the estimator is biased or unbiased? Is this a correct result?\n",
    "2. What if you use $N-1$ instead of $N$ in the denominator of the estimator, i.e. use an estimator based on the unbiased estimator of the variance?  \n",
    "3. Verify your results by estimating the bias of $\\hat\\sigma$ using random samples of proteins and the true value of the standard deviation of the log-length.  \n",
    "4. \\* Can you give a short argument on why this estimation of the bias works?  \n",
    "5. How does the bias of the estimator influence the length of the asymptotic confidence intervals - on average, are they too short, too long, or neither? What is the consequence for the actual confidence level of this type of interval?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_bootstrap = bootstrap_std - true_std\n",
    "print(f\"Bias Estimated from Bootstrap Replicates: {bias_bootstrap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_unbiased_denominator = np.std(bootstrap_std, ddof=1)\n",
    "print(f\"Standard Deviation with Unbiased Denominator: {std_unbiased_denominator}\")\n",
    "\n",
    "# Calculate the bias using the unbiased denominator\n",
    "bias_unbiased_denominator = std_unbiased_denominator - true_std\n",
    "print(f\"Bias with Unbiased Denominator: {bias_unbiased_denominator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_and_calculate_std(R, N):\n",
    "    std_values = []\n",
    "    for _ in range(R):\n",
    "        sample = human_protein_lengths['LogLength'].sample(N, replace=True)\n",
    "        std_values.append(np.std(sample, ddof=1))\n",
    "    return std_values\n",
    "\n",
    "std_values_unbiased_denominator = draw_samples_and_calculate_std(10000, N)\n",
    "\n",
    "# Calculate the bias using random samples and unbiased denominator\n",
    "bias_true_value_unbiased_denominator = np.mean(std_values_unbiased_denominator) - std_true_distribution\n",
    "print(f\"Bias Estimated from True Distribution with Unbiased Denominator: {bias_true_value_unbiased_denominator}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFDoCpVJMnPX"
   },
   "source": [
    "**Exercise 6.\\*\\*** One of the applications of estimating a distribution is performing hypothesis testing when it's difficult or impossible to calculate the distribution of a test statistic analytically. Recall that hypothesis testing proceeds as follows:  \n",
    "1. We set a null hypothesis $H_0$, and alternative hypothesis $H_1$, and a test statistic $T$ that will give the test a high statistical power.  \n",
    "2. We derive the distribution of $T$ assuming $H_0$ is true.  \n",
    "3. We calculate the observed value of the test statistic, $T^*$.  \n",
    "4. We define the critical region $C_{T^*}$ such that $T \\in C_{T^*}$ is unlikely under $H_0$, but more likely under $H_1$. Often, $C = \\{t \\in \\mathbb{R}: t \\leq -|T^*| \\text{ or } t \\geq |T^*|\\}$. The critical region is selected to get as high power of the test as possible.\n",
    "4. We calculate the p-value $p = \\mathbb{P}(T \\in C_{T^*} | H_0)$, i.e. the probability that $T$ falls into the critical region $C_{T^*}$ assuming $H_0$ is true. Low p-values indicate that $H_0$ is false. The *significance level* of the test is the threshold of the p-value below which we discard $H_0$.     \n",
    "\n",
    "We can use the bootstrap to complete step 2 when we don't know the distribution of the test statistic. To do this, we obtain bootstrap replicates of our statistical sample and calculate the value of $T$ in each replicate.  \n",
    "However, when we estimate the distribution of $T$, we need to ensure that $H_0$ is true. To do this, we usually need to transform our sample. We'll see how to do this on an example of testing the equality of mean log-lengths of proteins in two organisms.  \n",
    "\n",
    "1. Write a function that takes two vectors and returns the value of the test statistic $T$ of the [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test).  \n",
    "2. Select two random samples, each of size $N=20$, of protein log-lengths from two organisms.  \n",
    "3. Compute the observed value of the Welsch's test statistic, $T^*$, for these two samples.   \n",
    "4. Now, we will use the bootstrap to obtain the distribution of $T$ under the null hypothesis that the mean log-lengths are equal. Note that, in the following substeps, we equalize the means in the *samples*, not in the *populations* - we pretend that we don't have any other data than the two samples of size $N$, and that we don't know the true means. Therefore, the null hypothesis will be valid only asymptotically (i.e. for infinite sample sizes).\n",
    "  4.1. Calculate the mean log-lengths of proteins in both samples.  \n",
    "  4.2. Subtract the corresponding mean from each sample, so that both vectors of log-lengths have a zero mean.  \n",
    "  4.3. For 1000 times or more, resample the log-lengths from both vectors and compute the value of the test statistic $T$.  \n",
    "  4.4. Visualize the distribution of $T$ under the null hypothesis on a histogram.   \n",
    "5. Calculate the p-value by checking the proportion of bootstrapped values of $T$ that are at least as extreme as $T^*$ (for an alternative hypothesis of your choice). Do you reject the null hypothesis?\n",
    "6. Perform a Welch's t-test using the `scipy.ttest_ind` function and compare the results. Are the p-values similar?  \n",
    "  6.1. Note that you may use the output of this function to verify if you have computed $T^*$ properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEUt5rf0mZ8v"
   },
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mubln3ogmzrH"
   },
   "source": [
    "**Exercise 7.**  In Exercises 2 and 3 we've seen that seemingly reasonable models may perform worse if they have too many variables. This is because a high flexibility allows them to learn spurious signals in the training data set that do not generalize to new observations and decrease their performance. However, some of the additional variables may indeed be useful. How, then, do we pick the best set of features for our model?  \n",
    "\n",
    "In principle, we could inspect all the possible combinations of features. However, this is often prohibitively expensive computationally. Typically, we use one of the following heuristic approaches:  \n",
    "1. The *forward* model selection, where we start with an empty model and iteratively add one feature that gives the best improvement\n",
    "2. The *backwards* model selection, where we start with a full model (i.e. with all the features that we can take) and we iteratively remove one feature; we select the feature such that its removal gives the best improvement\n",
    "3. The *forward-backward* model selection, where we start with an empty model and iteratively either add or remove one feature, depending on which gives the best improvement  \n",
    "4. The *backward-forward* model selection, which you can figure out at this point.  \n",
    "\n",
    "All the procedures iterate either until we reach a desired number of variables, or until we can no longer improve our model. There are several methods to evaluate the improvement of our model that we get by adding or removing a feature. The most general one is the test error estimated with k-fold cross validation. In this exercise, we will use this technique to select the best model to predict city incomes in 2018.   \n",
    "\n",
    "1. Create a data frame `X` of the independent variables: incomes in 2015, 2016 and 2017, their second powers, and their products. Add an intercept, i.e. a constant column equal 1.   \n",
    "2. Create a `LinearRegression` object with `fit_intercept=False`. In this exercise we add the intercept manually, because we want to include it in the model selection procedure.  \n",
    "3. Use the `SequentialFeatureSelector` function from scipy to perform a forward model selection until adding a feature does not improve the model, with test error estimated using 10-fold cross validation. Look up the documentation of this function online in order to correctly specify the keyword arguments `n_features_to_select`, `cv` and `tol`.  \n",
    "  3.1. Inspect the new model. Estimate its test error with cross validation with the same $k$ as you used in Exercise 2 and 3. Did you get a better test error? If yes, does it mean that you have found the best set of features?       \n",
    "  3.2. Experiment with different values of the `tol` parameter. Did increasing or decreasing it allow you to find a better model? Why?  \n",
    "4. Now, perform a backwards model selection using `SequentialFeatureSelector`.  \n",
    "  4.1. Did you obtain the same model as in point 3? If not, which model is better?  \n",
    "  4.2. What are the possible advantages and disadvantages of the backward strategy compared to the forward strategy?   \n",
    "  4.3. Experiment with different values of the `tol` parameter. Did increasing or decreasing it allow you to find a better model? Why?  \n",
    "5. \\** Implement your own forward-backward procedure. Iterate until the difference between the new and the current model RMSE is not larger than 0.01 (i.e. we modify our model even if we get a slightly worse RMSE). We set this threshold in order to search more models - a temporary decrease in model performance may allow us to discover a better model.  \n",
    "  5.1. What risks do we run into when we can both increase and decrease model performance in consecutive steps? How to handle it?  \n",
    "  5.2. Use your implementation to select the best set of features for predicting city incomes. Was removing a feature beneficial at any step?  \n",
    "  5.3. Did you obtain a better performance than using a simple forward strategy?  \n",
    "  5.4. Did the error threshold of 0.01 allow you to find a better model compared to an error threshold equal zero (i.e. accepting a new model only if it improves over the current one)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = pd.DataFrame()\n",
    "X['2015'] = city_income['2015']\n",
    "X['2016'] = city_income['2016']\n",
    "X['2017'] = city_income['2017']\n",
    "X['2015^2'] = city_income['2015'] ** 2\n",
    "X['2016^2'] = city_income['2016'] ** 2\n",
    "X['2017^2'] = city_income['2017'] ** 2\n",
    "X['2015*2016'] = city_income['2015'] * city_income['2016']\n",
    "X['2015*2017'] = city_income['2015'] * city_income['2017']\n",
    "X['2017*2016'] = city_income['2017'] * city_income['2016']\n",
    "X['constant'] = 1  # intercept\n",
    "\n",
    "y = city_income[\"2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sfs_forward = SequentialFeatureSelector(model, n_features_to_select=None, direction='forward', cv=10, tol=0.01)\n",
    "sfs_forward.fit(X, y)\n",
    "selected_feature_indices = sfs_forward.get_support(indices=True)\n",
    "selected_features = X.columns[selected_feature_indices]\n",
    "print(\"Selected features from forward model selection:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_forward_selected = X[selected_features]\n",
    "cross_val_error_forward = np.mean(-cross_val_score(model, X_forward_selected, y, scoring='neg_mean_squared_error', cv=10))\n",
    "\n",
    "print(f\"Test error for forward model selection: {cross_val_error_forward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_backward = SequentialFeatureSelector(model, n_features_to_select=None, direction='backward', cv=10, tol=0.01)\n",
    "sfs_backward.fit(X, y)\n",
    "selected_feature_indices_backward = sfs_backward.get_support(indices=True)\n",
    "selected_features_backward = X.columns[selected_feature_indices_backward]\n",
    "\n",
    "print(\"Selected features from backward model selection:\", selected_features_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "- It starts with a full model, potentially considering interactions between variables.\n",
    "- It may be more conservative in removing features that don't add much predictive power.\n",
    "Disadvantages:\n",
    "\n",
    "- It can be computationally expensive for a large number of features.\n",
    "- It may remove useful features early if they don't show a strong individual effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPe-ApcjDZkc"
   },
   "source": [
    "**Exercise 8.** The city council of Bajtowo has lost the income data for 2017. Their income data base reads as follows:\n",
    "\n",
    "2015: 23 070 510.29 PLN  \n",
    "2016: 24 454 660.81 PLN  \n",
    "2017: ???  \n",
    "2018: 27 085 401.60 PLN  \n",
    "2019: 31 890 616.12 PLN    \n",
    "2020: 33 421 082.55 PLN    \n",
    "\n",
    "The mayor of Bajtowo has asked for your help. Create a model to predict the missing income and estimate its prediction error.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTJyoDbs95cb"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?export=view&id=12CrUdXDAiltLBT26sG7HZ_HciIhvGyT8'></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
