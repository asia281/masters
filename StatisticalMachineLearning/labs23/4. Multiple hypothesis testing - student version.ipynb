{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "685bcedd"
   },
   "source": [
    "# Statistical machine learning - Notebook 4, version for students\n",
    "**Author: Michał Ciach**  \n",
    "Exercises denoted with a star \\* are optional. They may be more difficult or time-consuming.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwGqN5_Q4162"
   },
   "source": [
    "## Data & library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7112,
     "status": "ok",
     "timestamp": 1696421225006,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "7c1qctHRoY_V",
    "outputId": "3274ad66-c86b-4184-b416-cd3501f9b486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/anaconda3/lib/python3.9/site-packages (4.7.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.9/site-packages (from gdown) (3.3.1)\r\n",
      "Requirement already satisfied: requests[socks] in /usr/local/anaconda3/lib/python3.9/site-packages (from gdown) (2.28.1)\r\n",
      "Requirement already satisfied: six in /usr/local/anaconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/anaconda3/lib/python3.9/site-packages (from gdown) (4.10.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.9/site-packages (from gdown) (4.62.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.2.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5060,
     "status": "ok",
     "timestamp": 1696421230060,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "jS4Mwv37_lgM",
    "outputId": "8bbf08c6-e7d0-46b5-e54b-93188d7b5034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0\n",
      "zsh:1: no matches found: https://drive.google.com/uc?id=1y5NKR3aWB0DbAuSWcg6ffa1Atu2unpOA\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1xOJfD-jexDbHSOCg1EiyAxqc5kXjMvX0\n",
    "!gdown https://drive.google.com/uc?id=1y5NKR3aWB0DbAuSWcg6ffa1Atu2unpOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1696421230060,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "1qclcOYOz3qg"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1696421232437,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "EkH7lfBv405O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.stats import t as tstud\n",
    "from scipy.stats import ttest_ind, ttest_rel, ttest_1samp, norm, kstest, mannwhitneyu, shapiro, chisquare, chi2_contingency\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1696421233149,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "QFOe8o1n41Ec",
    "outputId": "e6ba8473-9548-4ba1-e8f7-533dfab1b385"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scientific name</th>\n",
       "      <th>Common name</th>\n",
       "      <th>Protein ID</th>\n",
       "      <th>Protein length</th>\n",
       "      <th>LogLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>1474</td>\n",
       "      <td>3.168497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>290</td>\n",
       "      <td>2.462398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>421</td>\n",
       "      <td>2.624282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000008.1</td>\n",
       "      <td>412</td>\n",
       "      <td>2.614897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Homo sapiens</td>\n",
       "      <td>Human</td>\n",
       "      <td>NP_000009.1</td>\n",
       "      <td>655</td>\n",
       "      <td>2.816241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648731</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560453.1</td>\n",
       "      <td>494</td>\n",
       "      <td>2.693727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648732</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560454.1</td>\n",
       "      <td>737</td>\n",
       "      <td>2.867467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648733</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560455.1</td>\n",
       "      <td>554</td>\n",
       "      <td>2.743510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648734</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560456.1</td>\n",
       "      <td>813</td>\n",
       "      <td>2.910091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648735</th>\n",
       "      <td>Imleria badia</td>\n",
       "      <td>Bay bolete (mushroom)</td>\n",
       "      <td>KAF8560457.1</td>\n",
       "      <td>102</td>\n",
       "      <td>2.008600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648736 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Scientific name            Common name    Protein ID  Protein length  \\\n",
       "0         Homo sapiens                  Human   NP_000005.3            1474   \n",
       "1         Homo sapiens                  Human   NP_000006.2             290   \n",
       "2         Homo sapiens                  Human   NP_000007.1             421   \n",
       "3         Homo sapiens                  Human   NP_000008.1             412   \n",
       "4         Homo sapiens                  Human   NP_000009.1             655   \n",
       "...                ...                    ...           ...             ...   \n",
       "648731   Imleria badia  Bay bolete (mushroom)  KAF8560453.1             494   \n",
       "648732   Imleria badia  Bay bolete (mushroom)  KAF8560454.1             737   \n",
       "648733   Imleria badia  Bay bolete (mushroom)  KAF8560455.1             554   \n",
       "648734   Imleria badia  Bay bolete (mushroom)  KAF8560456.1             813   \n",
       "648735   Imleria badia  Bay bolete (mushroom)  KAF8560457.1             102   \n",
       "\n",
       "        LogLength  \n",
       "0        3.168497  \n",
       "1        2.462398  \n",
       "2        2.624282  \n",
       "3        2.614897  \n",
       "4        2.816241  \n",
       "...           ...  \n",
       "648731   2.693727  \n",
       "648732   2.867467  \n",
       "648733   2.743510  \n",
       "648734   2.910091  \n",
       "648735   2.008600  \n",
       "\n",
       "[648736 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_lengths = pd.read_csv('protein_lengths.tsv', sep='\\t')\n",
    "protein_lengths['LogLength'] = np.log10(protein_lengths['Protein length'])\n",
    "protein_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1696421233151,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "QVYnO3rm_sSc",
    "outputId": "55a02c04-5591-4d1a-987b-70800c838672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scientific name Common name   Protein ID  Protein length  LogLength\n",
      "0    Homo sapiens       Human  NP_000005.3            1474   3.168497\n",
      "1    Homo sapiens       Human  NP_000006.2             290   2.462398\n",
      "2    Homo sapiens       Human  NP_000007.1             421   2.624282\n",
      "3    Homo sapiens       Human  NP_000008.1             412   2.614897\n",
      "4    Homo sapiens       Human  NP_000009.1             655   2.816241\n",
      "\n",
      "       Protein length      LogLength\n",
      "count   136193.000000  136193.000000\n",
      "mean       692.655775       2.711540\n",
      "std        746.993628       0.329892\n",
      "min         12.000000       1.079181\n",
      "25%        316.000000       2.499687\n",
      "50%        514.000000       2.710963\n",
      "75%        842.000000       2.925312\n",
      "max      35991.000000       4.556194\n"
     ]
    }
   ],
   "source": [
    "human_protein_lengths = protein_lengths.loc[protein_lengths['Common name'] == 'Human'].copy()\n",
    "# Note: without .copy(), some versions of Pandas may return a View.\n",
    "# This may interfere with adding a new column to human_protein_lengths.\n",
    "print(human_protein_lengths.head())\n",
    "print()\n",
    "print(human_protein_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1696421233152,
     "user": {
      "displayName": "Michał Ciach",
      "userId": "00832457977902949614"
     },
     "user_tz": -120
    },
    "id": "Fs8D4zlT1pTm",
    "outputId": "8d6e4d7a-a1b1-44c6-b2a2-3edf4fa0f7a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>43806.926920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>62825.327197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>46660.685387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>54625.879014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>41976.428035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>B</td>\n",
       "      <td>55338.284370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>B</td>\n",
       "      <td>95699.796316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>B</td>\n",
       "      <td>34417.930506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>B</td>\n",
       "      <td>20637.868432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>B</td>\n",
       "      <td>24786.699022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country        Income\n",
       "0          A  43806.926920\n",
       "1          A  62825.327197\n",
       "2          A  46660.685387\n",
       "3          A  54625.879014\n",
       "4          A  41976.428035\n",
       "...      ...           ...\n",
       "1995       B  55338.284370\n",
       "1996       B  95699.796316\n",
       "1997       B  34417.930506\n",
       "1998       B  20637.868432\n",
       "1999       B  24786.699022\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citizen_incomes = pd.read_csv('citizen incomes.tsv', sep='\\t')\n",
    "citizen_incomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETOqr5vCrkEA"
   },
   "source": [
    "## Multiple hypothesis testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iILC41JzPcxc"
   },
   "source": [
    "When performing statistical tests, we reject the null hypothesis when the p-value is below a set threshold, traditionally 0.05. A consequence of this approach is that 5% of positive results will be false positives (Type I errors; incorrect rejections of a true null hypothesis), which may be a huge number when thousands of tests are performed in large-scale studies.  \n",
    "\n",
    "In order to limit the number of false positives, we use *multiple testing corrections*. One of the most useful ones is the Benjamini-Hochberg correction, which controls the False Discovery Rate (FDR), i.e. the proportion of false positives among all positive results. Note the difference from significance levels: the false discovery rate is the proportion of false positives among all positives in the *results* of the tests, while the significance level is the proportion of false positives among the *true negatives* (cases when $H_0$ is true).  \n",
    "\n",
    "The assumptions and details of the Benjamini-Hochberg correction (and other common corrections) were discussed in the lecture; you may want to review them now. One of the most important assumptions is that the p-values come from a set of independent tests.  \n",
    "\n",
    "A common way to perform the Benjamini-Hochberg correction is to transform the p-values $p_1, \\dots, p_m$ in a way to obtain so-called *q-values* $q_1, \\dots, q_m$, such that we have an FDR on the level of $Q$ when we accept all hypotheses $H_i$ with $q_i \\leq Q$.\n",
    "\n",
    "\n",
    "Do not confuse the False Discovery Rate (FDR) with other similarly named metrics, like the False Positive Rate (FPR)! FDR is the ratio of the number of false positives to the number of all reported positives, while FPR is the ratio of the number of false positives to the number of true null hypotheses. You can see a list of other common metrics [here](https://en.wikipedia.org/wiki/Confusion_matrix).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKBr6c7ttByS"
   },
   "source": [
    "In the next exercise, we will use *stochastic simulations* to generate our own data. First, however, let's talk about what it means to simulate a random variable.  \n",
    "\n",
    "*Theory time.* The term *simulating a random variable* is confusing for many students, especially the ones that paid attention in probability classes. Recall that a random variable $X$ is a function from the space of elementary outcomes $\\Omega$ to the space of real (or integer) numbers. When some students hear \"let's simulate a sequence of random variables $X_1, \\dots, X_n$, they wonder: how do we simulate a sequence of functions? Well, this is not what we mean at all.   \n",
    "\n",
    "We define random variables as functions because we want to have a mathematical model of randomness. This is a convenient way to rigorously define what it means that a variable $X$ takes some value with some probability. For example, we can use it to formally define $\\mathbb{P}(X=1) = \\mathbb{P}(\\{ \\omega: X(\\omega)  =1 \\})$. We need this definition, because the concept of probability is defined on the space $\\Omega$, not on the real numbers. However, this is not how we interpret or use random variables in practice.\n",
    "\n",
    "Recall that the number of dots on a set of dice is a sequence of random variables $X_1, \\dots, X_n$ before you throw the dice. When you throw them, you get realizations of the random variables, $X_1(\\omega), \\dots, X_n(\\omega)$, which are a sequence of numbers (and often called *random numbers*). Measuring the outcome of a random experiment corresponds to fixing $\\omega$ in a sequence of random variables. This is why we can interpret random variables as numerical variables. Defining them as functions is necessary only because otherwise terms like $\\mathbb{P}(X=1)$ don't have any mathematical meaning.\n",
    "\n",
    "\"Fixing $\\omega$\" is also what we mean by simulating a sequence of random variables: we want to get a sequence of numbers $x_1, \\dots, x_n$ that are realizations of a sequence of random variables $X_1, \\dots X_n$, so that $x_i = X_i(\\omega)$. However, simulating doesn't mean that we pick $\\omega$ by hand. Remember that $\\omega$ is mostly a mathematical model, not a physical being. Instead of selecting it by hand, we rely on random number generators that \"fix $\\omega$\" for us.   \n",
    "\n",
    "You have already encountered a few random number generators. A set of dice is one of them. When you throw the dice, you can imagine that the action of throwing corresponds to fixing $\\omega$ in a sequence of random variables - and you get a sequence of numbers as a result. You probably have also used pseudo-random number generators in your programs. These are algorithms that return pseudo-random numbers between 0 and 1. When you run this algorithm, you can imagine that this corresponds to fixing $\\omega$ in a random variable $X\\sim U([0, 1])$ - and you get a random number as a result.   \n",
    "\n",
    "These two random number generators (the dice and the algorithm) return numbers with very simple distributions. However, we can simulate random numbers with many other distributions, such as the Normal one. There are many techniques to do this, which usually rely on transforming random numbers distributed uniformly between 0 and 1 (formally speaking, numbers which are a realization of a sequence of random variables $X_i$ such that $X_i \\sim_{iid} U([0, 1])$). In this course, you don't need to worry about these techniques: we will simulate random variables using algorithms which are already implemented in Python libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSM59hWBC2NP"
   },
   "source": [
    "**Exercise 1.** In this exercise, we'll learn how to use the Benjamini-Hochberg correction to control the False Discovery Rate, as well as how to simulate our own data sets. We will generate a set of samples from two different distributions and test for the value of their means, resulting in a set of tests in which some null hypotheses are true, and some are false.  \n",
    "\n",
    "1. Generate $M=1000$ random samples of size $N=10$ from the standard normal distribution, and $M=1000$ samples of size $N=10$ from the normal distribution with expected value $\\mu=1$. Use the `norm.rvs` function from `scipy` (if you need to, you can look for its documentation online).\n",
    "2. For each sample, do a one-sample Student's t test to check whether the mean is equal zero. Save the p-values.      \n",
    "  2.1. How many null hypotheses are true, and how many are false?   \n",
    "3. Reject the null hypotheses on the significance level 5% (i.e. when the p-values are less than 0.05).   \n",
    "  3.1. How many positive results (i.e. rejections of the null hypothesis) did you get?  \n",
    "  3.2. How many true positives did you get?  \n",
    "  3.3. How many false positives did you get?  \n",
    "  3.4. How many false positives did you expect given the significance level?   \n",
    "  3.5. Calculate the obtained False Positive Rate. Is it close to the significance level? Why/why not?       \n",
    "  3.6. Calculate the obtained False Discovery Rate.     \n",
    "  3.7. Which of the previous points depend on $N$? Which do depend on $M$? Give a theoretical argument.   \n",
    "4. Perform the Benjamini-Hochberg correction on your p-values using the `fdrcorrection` function from the `statsmodels` library. Reject the null hypothesis at the FDR level 10%.  \n",
    "  4.1. How many positive results did you get?  \n",
    "  4.2. How many true positives did you get?  \n",
    "  4.3. How many false positives did you get?  \n",
    "  4.4. Calculate the obtained False Positive Rate. Is it close to the significance level? Why/why not?     \n",
    "  4.5. Calculate the obtained False Discovery Rate. Is it close to the assumed 10% FDR? Why/why not?  \n",
    "  4.6. How many false positives did you expect given the FDR level?  \n",
    "5. Suppose all the samples were drawn from the standard normal distribution. What would be the effect of applying the Benjamini-Hochberg correction? How many false positives would you expect to obtain?    \n",
    "6. \\* Based on the description of the Benajmini-Hochberg procedure from [this Wikipedia article](https://en.wikipedia.org/wiki/False_discovery_rate#Benjamini%E2%80%93Hochberg_procedure), figure out how to compute the q-values given a list of p-values. Write a function that accepts a vector of p-values and returns the corresponding q-values. Compare your results to the `fdrcorrection` function from the `statsmodels` library on a p-value vector (0.01, 0.1, 0.01, 0.2, 0.01, 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "M = 1000\n",
    "N = 10\n",
    "\n",
    "samples_standard = norm.rvs(size=(M, N))\n",
    "samples_mean_1 = norm.rvs(loc=1, size=(M, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(dataset):\n",
    "    return np.array([ttest_1samp(sample, 0).pvalue for sample in dataset])\n",
    "\n",
    "def calculate_stats(p_values, alpha = 0.05):\n",
    "    reject = np.array(p_values) < alpha\n",
    "    true_positives = np.sum(reject)\n",
    "    false_positives = np.sum(reject)\n",
    "    # 3.4,5,6\n",
    "    expected_false_positives = M * alpha\n",
    "    fpr = false_positives / M\n",
    "    fdr = false_positives / max(1, true_positives)\n",
    "    tpr = true_positives / M\n",
    "    return tpr, fpr, fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "p_values_standard = ttest(samples_standard)\n",
    "p_values_mean_1 = ttest(samples_mean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "_, fpr, fdr = calculate_stats(p_values_standard)\n",
    "_, fpr, fdr = calculate_stats(p_values_mean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y5/g_1zz7c13m3drp5b8lv_zk0m0000gn/T/ipykernel_3940/1694623937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_values_standard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdrcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_values_standard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_values_mean_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdrcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_values_mean_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_values_standard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "_, p_values_standard = fdrcorrection(p_values_standard, alpha=0.1)\n",
    "_, p_values_mean_1 = fdrcorrection(p_values_mean_1, alpha=0.1)\n",
    "\n",
    "_, fpr, fdr = calculate_stats(p_values_standard, alpha=0.1)\n",
    "_, fpr, fdr = calculate_stats(p_values_mean_1, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all samples are drawn from the standard normal distribution, no true positives are expected\n",
    "# Applying Benjamini-Hochberg correction should result in zero false positives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_values(p_values):\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    ranked_p_values = p_values[sorted_indices]\n",
    "    q_values = alpha * np.arange(1, len(p_values) + 1) / len(p_values)\n",
    "    return q_values[np.argsort(sorted_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example p-values vector\n",
    "example_p_values = np.array([0.01, 0.1, 0.01, 0.2, 0.01, 0.1])\n",
    "\n",
    "# Compute q-values using the custom function\n",
    "example_q_values_custom = q_values(example_p_values)\n",
    "\n",
    "# Compute q-values using statsmodels fdrcorrection function\n",
    "_, example_q_values_statsmodels, _, _ = fdrcorrection(example_p_values, alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzz2rXQilVfJ"
   },
   "source": [
    "**Exercise 2.\\*** In the previous exercise, you may have noticed that the results vary between the runs. Each sampling of the $2M$ samples returns a different FDR. In this exercise, we'll analyze the distribution of the FDR.  \n",
    "1. Repeat the calculation of the \"raw\" FDR (i.e. before the correction) and the \"BH\" FDR (after the correction) for $R=1000$ times. Also calculate the True Positive Rate, i.e. the fraction of correctly identified positives.  \n",
    "2. Generate histograms that compare the FDR and TPR before and after the BH correction.     \n",
    "  2.1. Is there any adverse effect to using the Benjamini-Hochberg correction?  \n",
    "3. Calculate the average FDR after the correction. Is it close to the assumed 10% level? Why/why not?       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "R = 1000 \n",
    "raw_fdr = []\n",
    "bh_fdr = []\n",
    "tpr = []\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for _ in range(R):\n",
    "    samples_standard = np.random.randn(M, N)\n",
    "    samples_mean_1 = np.random.normal(loc=1, size=(M, N))\n",
    "    tpr_sample, fdr_sample, bh_sample = \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2iJxmlLiu0e"
   },
   "source": [
    "**Exercise 3.** In each of the research experiments below, there is either some procedure applied incorrectly, or it is correct but can be improved. For each point, say whether all the assumptions are satisfied, and whether the methodology can be improved (e.g. by using a different statistical test or correction procedure). If the assumptions are violated, discuss the possible effect of this kind of violation. For extra points, support your claims with theoretical arguments or simulations.        \n",
    "1. A researcher studies the effect of glucose consumption on rat well-being. The researcher has partitioned $N=20$ rats into two groups: an experimental one with excess glucose in the diet, and a control one with a normal diet. After a week, the researcher has measured the weight of the rats, the length of sleep, the blood glucose level, the abdominal circumference, the body fat percentage, and the anxiety level (using the elevated plus maze assay). The researcher has compared each factor between the experimental and the control group using the two-sample Student's t test for unpaired observations, previously verifying that the compared variables are normally distributed using the Shapiro-Wilk's test. Next, the researcher has corrected the Student's t test p-values for multiple testing using the Benjamini-Hochberg correction at FDR level 20%.   \n",
    "2. A researcher studies the effect of consumption of small amounts of vinegar on the increase of the blood glucose level (BGC) in mice. The researcher has partitioned $N=20$ mice into equally-sized experimental and control groups. At the beginning of the experiment, he has measured the BGC of all mice. Next, for four days, mice from both group were given a diet enriched in glucose. Mice from the experimental groups were also given small amounts of vinegar in their diet. After four days, the BGC of all mice was measured again. Next, in each group, the researcher has tested for the increase in BGC using a two-sample Student's t-test for unpaired observations.  \n",
    "3. A researcher studies the effect of artificial sweeteners on the blood glucose level (BGC) in mice. The researcher has partitioned $N=100$ mice into ten equally-sized groups: nine experimental and one control. Mice in the control group were given a normal diet. In the experimental groups, the mice were given food enriched in glucose; sucralose; aspartam; erythritol; stevia; xylitol; saccharine; acesulfam K; manninol. After four days, the researcher has measured the BGC of all mice. Next, the researcher has used the two-sample Student's t test for unpaired observations to compare the average BGC between the ten groups. To correct for multiple tests, the researcher has used the Benjamini-Hochberg procedure at FDR level 20%.      \n",
    "4. A researcher studies the effect of salt consumption on the abdominal circumference (AC) in mice. The researcher has partitioned $N=20$ mice into equally-sized experimental and control groups. The AC of all mice was measured at the beginning of the experiment. Mice from the experimental group were given a diet enriched in salt. After a week, the AC was measured again, and the researcher used a two-sample Student's t test for paired observations in each group. The results of the test in each group were insigificant, and the researcher concluded that salt consumption does not increase the abdominal circumference in the span of one week.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxQeLgYTdwbf"
   },
   "source": [
    "*Acknowledgement.*  \n",
    "Exercise 3 was partly inspired by the following resource: https://stats.libretexts.org/Bookshelves/Applied_Statistics/Biological_Statistics_(McDonald)/06%3A_Multiple_Tests/6.01%3A_Multiple_Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M060dORLUS7a"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?export=view&id=12CrUdXDAiltLBT26sG7HZ_HciIhvGyT8'></center>|"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
