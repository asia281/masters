{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/dnn2022/blob/main/Asia_of_DNN_Lab_4_MNIST_in_Pytorch_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxW4dJFDfX_a"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6cY1NBlUOBx4"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, sizes: List):\n",
        "        super(Net, self).__init__()\n",
        "        # After flattening an image of size 28x28 we have 784 inputs\n",
        "        self.layers = nn.ModuleList([nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        for layer in self.layers[:-1]:\n",
        "          x = layer(x)\n",
        "          x = F.relu(x)\n",
        "        x = self.layers[-1](x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P1fFUewRRbdQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(epochs, losses):\n",
        "    plt.subplots(1, figsize=(10,10))\n",
        "    # plt.plot(train_sizes, np.mean, '--', color=\"#111111\",  label=\"Training score\")\n",
        "    plt.plot(epochs, losses, color=\"#111111\", label=\"Cross-validation score\")\n",
        "\n",
        "    plt.title(\"Learning Curve\")\n",
        "    plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    return losses\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "def update_kwargs(use_cuda, seed, batch_size, test_batch_size):\n",
        "  use_cuda = not use_cuda and torch.cuda.is_available()\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  train_kwargs = {'batch_size': batch_size}\n",
        "  test_kwargs = {'batch_size': test_batch_size}\n",
        "  if use_cuda:\n",
        "      cuda_kwargs = {'num_workers': 1,\n",
        "                      'pin_memory': True,\n",
        "                      'shuffle': True}\n",
        "      train_kwargs.update(cuda_kwargs)\n",
        "      test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "  return train_kwargs, test_kwargs, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o0KPoUtsfBOs"
      },
      "outputs": [],
      "source": [
        "def transform(train_kwargs, test_kwargs):\n",
        "  transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307,), (0.3081,))\n",
        "      ])\n",
        "  dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                      transform=transform)\n",
        "  dataset2 = datasets.MNIST('../data', train=False,\n",
        "                      transform=transform)\n",
        "  \n",
        "  train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ezvIQbgsfBRT"
      },
      "outputs": [],
      "source": [
        "def run_experiment(batch_size, test_batch_size, epochs, lr, log_interval, sizes, MomentOptimizer = False):\n",
        "  train_kwargs, test_kwargs, device = update_kwargs(use_cuda = False, seed=1, batch_size=batch_size, test_batch_size=test_batch_size)\n",
        "  model = Net(sizes).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "  if MomentOptimizer:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "  train_loader, test_loader = transform(train_kwargs, test_kwargs)\n",
        "  losses_test = []\n",
        "  epoch_list = [i for i in range(1175)]\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "      losses_test.extend(test(model, device, test_loader))\n",
        "\n",
        "  plot_learning_curve(epoch_list, losses_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K5GlMs1-fBKP"
      },
      "outputs": [],
      "source": [
        "sizes = [784, 128, 128, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "8MZ9VeqCIGze",
        "outputId": "f0714463-a2b3-4298-ded4-8789c4ae863f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313259\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.732760\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.582762\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.481087\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.334523\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.346265\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.229349\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.191099\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.224889\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.244065\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.236049\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.241033\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.194209\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.302404\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.203934\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.192445\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.173173\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.140804\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.356528\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.130329\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.193252\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.220827\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.161579\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.033481\n",
            "\n",
            "Test set: Average loss: 0.2275, Accuracy: 9311/10000 (93%)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-79f98d3cb3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-fa79dc506feb>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(batch_size, test_batch_size, epochs, lr, log_interval, sizes, MomentOptimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mlosses_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 1e-2, sizes=sizes, log_interval = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upPAhgRNVkVz"
      },
      "outputs": [],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 1e-3, sizes=sizes, log_interval = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdtbluujWBHr"
      },
      "outputs": [],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 6e-4, sizes=sizes, log_interval = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COUSJpmNHWzB"
      },
      "outputs": [],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 1e-2, log_interval = 10, sizes=sizes, MomentOptimizer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj4qMeZZbnps"
      },
      "outputs": [],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 2e-2, log_interval = 10, sizes=sizes, MomentOptimizer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQNN_VgElDC1"
      },
      "outputs": [],
      "source": [
        "run_experiment(batch_size = 256, test_batch_size = 1000, epochs = 5, lr = 3e-2, log_interval = 10, sizes=sizes, MomentOptimizer=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise we are using high level abstractions from torch.nn like nn.Linear.\n",
        "Note: during the next lab session we will go one level deeper and implement more things\n",
        "with bare hands.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Read the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 95% test accuracy for architecture input-128-128-10 after few epochs.\n",
        "\n",
        "    3. Add the option to use SGD with momentum instead of ADAM.\n",
        "\n",
        "    4. Experiment with different learning rates, plot the learning curves for different\n",
        "    learning rates for both ADAM and SGD with momentum.\n",
        "\n",
        "    5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
        "    Note that this requires creating a list of layers as an atribute of the Net class,\n",
        "    and one can't use a standard python list containing nn.Modules (why?).\n",
        "    Check torch.nn.ModuleList.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}